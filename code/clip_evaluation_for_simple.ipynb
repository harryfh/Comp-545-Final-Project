{"cells":[{"cell_type":"markdown","metadata":{},"source":["Modified from https://github.com/McGill-NLP/imagecode/blob/main/baselines/clip/evaluate_clip.py\n","\n","Command line arguments are removed and replaced with hard coded values."]},{"cell_type":"markdown","metadata":{},"source":["## Evaluating Clip Non-Contrastive and Contrastive"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install torch torchvision \n","!pip install git+https://github.com/openai/CLIP.git \n"]},{"cell_type":"code","execution_count":35,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-22T18:16:40.968835Z","iopub.status.busy":"2024-04-22T18:16:40.967862Z","iopub.status.idle":"2024-04-22T18:18:51.423888Z","shell.execute_reply":"2024-04-22T18:18:51.422915Z","shell.execute_reply.started":"2024-04-22T18:16:40.968796Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["USING DEVICE: cuda\n","2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2302/2302 [01:49<00:00, 20.99it/s]"]},{"name":"stdout","output_type":"stream","text":["OVERALL ACC: 0.6338\n","VIDEO ACC: 0.5908\n","IMG ACC: 0.8209\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","import json\n","from collections import defaultdict\n","from glob import glob\n","import os\n","import numpy as np\n","import clip\n","import torch\n","from PIL import Image\n","from pathlib import Path\n","import statistics\n","import argparse\n","\n","def encode_images(photos_batch):\n","    photos = [Image.open(photo_file) for photo_file in photos_batch]\n","    photos_preprocessed = torch.stack([preprocess(photo) for photo in photos]).to(device)\n","\n","    with torch.no_grad():\n","        photos_features = model.encode_image(photos_preprocessed)\n","        photos_features /= photos_features.norm(dim=-1, keepdim=True)\n","    return photos_features.cpu().numpy()\n","\n","\n","def encode_text(search_query):\n","    with torch.no_grad():\n","        text_encoded = model.encode_text(clip.tokenize(search_query, truncate=True).to(device))\n","        text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n","    return text_encoded.cpu().numpy()\n","\n","\n","def find_best_matches(text_features, photo_features):\n","    similarities = (photo_features @ text_features.T).squeeze(1)\n","    best_photo_idx = (-similarities).argsort()\n","    similarities = -similarities\n","    similarities.sort()\n","    return best_photo_idx, similarities\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f'USING DEVICE: {device}')\n","model, preprocess = clip.load('ViT-B/16', device=device, jit=False )  # Must set jit=False for training\n","\n","checkpoint = torch.load('/kaggle/input/imagecode-checkpoints/NOCONTRA_clip_best__36_4e-6_ViT-B16_--job_id_1258699.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(checkpoint['epoch'])\n","clip.model.convert_weights(model)  # Actually this line is unnecessary since clip by default already on float16\n","model.eval()\n","\n","correct = 0\n","total = 0\n","vid_correct = 0\n","vid_total = 0\n","img_correct= 0\n","img_total = 0\n","\n","\n","img_dirs = '/kaggle/input/imagecode-simple/image-sets/image-sets'\n","descriptions = json.load(open('/kaggle/input/imagecode-simple/valid_simple.json', 'r'))\n","\n","valid = []\n","for data in descriptions:\n","    valid.append((data['directory'], data['pos_idx'],data['neg_idx'], data['caption']))\n","\n","results = defaultdict(dict)\n","for img_dir, pos_idx, neg_idx, text in tqdm(valid):\n","    text = [text]\n","    pos_idx = int(pos_idx)\n","    img_files = list((Path(img_dirs) / img_dir).glob(\"*.jpg\"))\n","    img_files = sorted(img_files, key=lambda x: int(str(x).split('/')[-1].split('.')[0][3:]))\n","    img_files = [img_files[pos_idx], img_files[neg_idx]]\n","    \n","    images = [Image.open(photo_file) for photo_file in img_files]\n","    images = torch.stack([preprocess(photo) for photo in images]).to(device)\n","    text = clip.tokenize(text, truncate=True).to(device)\n","    with torch.no_grad():\n","        image_features = model.encode_image(images)\n","        text_features = model.encode_text(text)\n","        # normalized features\n","        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n","\n","        logits = (image_features @ text_features.T).squeeze(1)\n","    \n","    pred = torch.argmax(logits).squeeze()\n","\n","\n","    if pred.item() == 0:\n","        correct += 1\n","    if 'open-images' in img_dir:\n","        img_total += 1\n","        if pred.item() == 0:\n","            img_correct += 1\n","    else:\n","        vid_total += 1\n","        if pred.item() == 0:\n","            vid_correct += 1     \n","    results[img_dir].update({f'raw_preds_{pos_idx}': logits.squeeze().tolist(), f'clip_pred_{pos_idx}': int(pred.item()) ,f'correct_{pos_idx}': 1 if pos_idx == pred else 0})\n","\n","\n","print('OVERALL ACC: ' + str(round(correct/len(valid),4)))\n","print('VIDEO ACC: ' + str(round(vid_correct/vid_total,4)))\n","print('IMG ACC: ' + str(round(img_correct/img_total,4)))\n","json.dump(results, open(f'nocontra-clip-valid-simple-data.json', 'w'), indent=2)"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluating Contextual and Temporal Contextual\n","\n","Here we augment the evaluation_contextual.py in a similiar way that we did above. Instead of a classification task with 10 images, the problem is reduced to a binary classification."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T17:18:09.135524Z","iopub.status.busy":"2024-04-22T17:18:09.135116Z","iopub.status.idle":"2024-04-22T17:18:12.025955Z","shell.execute_reply":"2024-04-22T17:18:12.024625Z","shell.execute_reply.started":"2024-04-22T17:18:09.135490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'imagecode'...\n","remote: Enumerating objects: 589, done.\u001b[K\n","remote: Counting objects: 100% (55/55), done.\u001b[K\n","remote: Compressing objects: 100% (18/18), done.\u001b[K\n","remote: Total 589 (delta 45), reused 37 (delta 37), pack-reused 534\u001b[K\n","Receiving objects: 100% (589/589), 18.64 MiB | 18.63 MiB/s, done.\n","Resolving deltas: 100% (290/290), done.\n"]}],"source":["!git clone https://github.com/McGill-NLP/imagecode.git"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd /imagecode/baselines/clip"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T18:26:36.879794Z","iopub.status.busy":"2024-04-22T18:26:36.879375Z","iopub.status.idle":"2024-04-22T18:33:04.012459Z","shell.execute_reply":"2024-04-22T18:33:04.011506Z","shell.execute_reply.started":"2024-04-22T18:26:36.879762Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/imagecode/baselines/clip\n","DEVICE USED: cuda\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2302/2302 [06:22<00:00,  6.02it/s]"]},{"name":"stdout","output_type":"stream","text":["OVERALL ACC: 0.7033\n","VIDEO ACC: 0.6587\n","IMG ACC: 0.8977\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# inspired from: https://github.com/openai/CLIP/issues/83\n","# https://github.com/openai/CLIP/issues/83\n","import json\n","import os\n","import random\n","import wandb\n","import clip\n","from clip import model\n","import torch\n","from torch import autograd\n","import tqdm\n","from torch import nn, optim\n","from PIL import Image\n","from pathlib import Path\n","from collections import defaultdict\n","import sys\n","import argparse\n","print(os.getcwd())\n","from volta_src.config import BertConfig\n","from volta_src.embeddings import BertLayerNorm\n","from volta_src.encoders import GeLU\n","from extras import convert_sents_to_features, BertLayer\n","\n","\n","random.seed(10)\n","torch.manual_seed(10)\n","\n","def find_best_matches(text_features, photo_features):\n","    similarities = (photo_features @ text_features.T).squeeze(1)\n","    best_photo_idx = (-similarities).argsort()\n","    similarities = -similarities\n","    similarities.sort()\n","    return best_photo_idx, similarities\n","\n","\n","def convert_models_to_fp32(model):\n","    for p in model.parameters():\n","        if p.grad is not None:\n","            p.data = p.data.float()\n","            p.grad.data = p.grad.data.float()\n","\n","class ContextualCLIP(torch.nn.Module):\n","    def __init__(self, bert_config, args):\n","        super(ContextualCLIP, self).__init__()\n","        self.clip, self.preprocess = clip.load('ViT-B/16', device=device, jit=False)\n","        config = BertConfig.from_dict(bert_config)\n","        self.fusion = args.fusion\n","        if self.fusion == 'concat':\n","            hidden_size = 1024\n","        else:\n","            hidden_size = 512\n","\n","        config.hidden_size =  hidden_size\n","        config.num_attention_heads = 8\n","        self.transformer = nn.ModuleList([BertLayer(config) for _ in range(args.transformer_layers)])\n","        self.transformer.cuda()\n","        self.prediction_layer = nn.Linear(config.hidden_size, 1).cuda()\n","        self.batch_size = 1\n","        self.logit_scale = float(args.logit_scale)\n","        self.frozen_clip = args.frozen_clip\n","        self.add_input = args.add_input\n","        self.positional = args.positional\n","        if args.positional:\n","            self.positional_emb = torch.nn.Embedding(10,hidden_size).cuda()\n","\n","    def forward(self, images, text, pos_mask):\n","        if self.frozen_clip:\n","            with torch.no_grad():\n","                image_features = self.clip.encode_image(images)\n","                text_features = self.clip.encode_text(text)\n","        else:\n","            image_features = self.clip.encode_image(images)\n","            text_features = self.clip.encode_text(text)\n","        # normalized features\n","        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n","        text_features = torch.cat(10 * [text_features])\n","        if self.fusion == 'concat':\n","            x = torch.cat((image_features, text_features), dim=1)\n","        else:\n","            x = (self.logit_scale * image_features) * text_features\n","        x_ = torch.unsqueeze(x,dim=0)\n","        if self.positional:\n","            embs = self.positional_emb(torch.arange(10).cuda())\n","            embs = embs * pos_mask\n","            x_pos = x_ + embs\n","        else:\n","            x_pos = x_\n","        attention_mask = torch.ones((self.batch_size,1,1,10)).cuda()\n","        x = self.transformer[0](x_pos, attention_mask)\n","        for layer_module in self.transformer[1:]:\n","            x = layer_module(x, attention_mask) #TODO: remove hard-coding of 10\n","        if self.add_input:\n","            x = x + x_\n","        preds = self.prediction_layer(x.half())\n","        return preds\n","\n","    def encode_images(self, photos_batch):\n","        photos = [Image.open(photo_file) for photo_file in photos_batch]\n","        photos_preprocessed = torch.stack([self.preprocess(photo) for photo in photos]).to(device)\n","\n","        with torch.no_grad():\n","            photos_features = self.clip.encode_image(photos_preprocessed)\n","            photos_features /= photos_features.norm(dim=-1, keepdim=True)\n","        return photos_features.cpu().numpy()\n","\n","    def encode_text(self, search_query):\n","        with torch.no_grad():\n","            text_encoded = self.clip.encode_text(clip.tokenize(search_query, truncate=True).to(device))\n","            text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n","        return text_encoded.cpu().numpy()\n","\n","\n","\n","ARGS = {\n","    'checkpoint': '/kaggle/input/imagecode-checkpoints/TEMP-CONTEXTUAL_clip_best__36_0.0001_2e-06_ViT-B16_mult_gelu_1000_True_True_True_True_1.0_1.0_2_1252131.pt',\n","    'test_descr_path': '/kaggle/input/imagecode-simple/valid_simple.json',\n","    'imgs_path': '/kaggle/input/imagecode-simple/image-sets/image-sets',\n","    'batchsize': 36,\n","    'fusion': 'mult',\n","    'activation': 'gelu',\n","    'logit_scale': 1000,\n","    'frozen_clip': True,\n","    'add_input': True,\n","    'positional': True,\n","    'head_scheduler': 1.0,\n","    'base_scheduler': 1.0,\n","    'transformer_layers': 2\n","}\n","args = argparse.Namespace(**ARGS)\n","assert args.fusion in ['concat', 'mult']\n","assert args.activation in ['leaky-relu', 'relu', 'gelu']\n","\n","\n","img_dirs = args.imgs_path\n","valid_data = json.load(open(args.test_descr_path, 'r'))\n","valid = []\n","for data in valid_data:\n","    valid.append((data['directory'], data['pos_idx'],data['neg_idx'], data['caption']))\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f'DEVICE USED: {device}')\n","\n","bert_config = json.load(open('vilbert-and-bert-config.json', 'r'))\n","contextual_clip = ContextualCLIP(bert_config, args)\n","checkpoint = torch.load(args.checkpoint)\n","contextual_clip.load_state_dict(checkpoint['model_state_dict'])\n","\n","\n","if device == \"cpu\":\n","    contextual_clip.float()\n","else:\n","    clip.model.convert_weights(\n","        contextual_clip)  # Actually this line is unnecessary since clip by default already on float16\n","\n","\n","correct = 0\n","total = 0\n","vid_correct = 0\n","vid_total = 0\n","img_correct= 0\n","img_total = 0\n","\n","results = defaultdict(dict)\n","for img_dir, pos_idx, neg_idx, text in tqdm.tqdm(valid):\n","    text = [text]\n","    img_idx = int(pos_idx)\n","    img_files = list((Path(img_dirs) / img_dir).glob(\"*.jpg\"))\n","    img_files = sorted(img_files, key=lambda x: int(str(x).split('/')[-1].split('.')[0][3:]))\n","    # Only a binary case for simple, instead of changing anything, just repeat the same image 5 times to\n","    # maintain the original positional embeddings from the checkpoints provided\n","    img_files = [img_files[pos_idx]] * 5 + [img_files[neg_idx]] * 5 \n","    images = [Image.open(photo_file) for photo_file in img_files]\n","    images = torch.stack([contextual_clip.preprocess(photo) for photo in images]).to(device)\n","    text = clip.tokenize(text, truncate=True).to(device)\n","    if \"open-images\" in str(img_dir):\n","        pos_mask = torch.zeros((10,1)).cuda() # Change 10 to 1\n","    else:\n","        pos_mask = torch.ones((10,1)).cuda() # Change 10 to 1\n","    with torch.no_grad():\n","        logits = contextual_clip(images, text, pos_mask).squeeze()\n","    pred = torch.argmax(logits).squeeze()\n","    # Correct is always in the first half now\n","    if pred.item() < 5:\n","        correct += 1\n","    if 'open-images' in img_dir:\n","        img_total += 1\n","        if pred.item() < 5:\n","            img_correct += 1\n","    else:\n","        vid_total += 1\n","        if pred.item() < 5:\n","            vid_correct += 1     \n","    total += 1\n","    results[img_dir].update({f'raw_preds_{img_idx}': logits.squeeze().tolist(), f'clip_pred_{img_idx}': int(pred.item()) ,f'correct_{img_idx}': 1 if img_idx == pred else 0})\n","\n","print('OVERALL ACC: ' + str(round(correct/len(valid),4)))\n","print('VIDEO ACC: ' + str(round(vid_correct/vid_total,4)))\n","print('IMG ACC: ' + str(round(img_correct/img_total,4)))\n","\n","json.dump(results, open(f'CONTEXTUAL_valid_simple.json', 'w'), indent=2)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4652364,"sourceId":7917611,"sourceType":"datasetVersion"},{"datasetId":4842738,"sourceId":8197343,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
