{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Helper functions to parse the ImageCoDe-simple validation data."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T04:22:03.048139Z","iopub.status.busy":"2024-04-23T04:22:03.047478Z","iopub.status.idle":"2024-04-23T04:22:03.055967Z","shell.execute_reply":"2024-04-23T04:22:03.054967Z","shell.execute_reply.started":"2024-04-23T04:22:03.048109Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","def load_dataset(datapath):\n","    with open(datapath, 'r') as f:  # Open file with 'r' for read mode\n","        data = json.load(f)\n","    return data\n","\n","def get_image_caption_pair():\n","    dataset = load_dataset('/kaggle/input/imagecode-simple/valid_simple.json')\n","    imagepath = '/kaggle/input/imagecode-simple/image-sets/image-sets'\n","    for data in dataset:\n","        directory, pos_idx, neg_idx, caption = data.values()\n","        file_list = sorted([file for file in os.listdir(f'{imagepath}/{directory}')], key=lambda x: int(x.split('.')[0][3:]))\n","        pos_img = f'{imagepath}/{directory}/{file_list[pos_idx]}'\n","        neg_img = f'{imagepath}/{directory}/{file_list[neg_idx]}'\n","\n","        yield pos_img,neg_img, caption"]},{"cell_type":"markdown","metadata":{},"source":["## Load the MMICL-tiny model\n","Note, you must first run the MMICL helper functions defined below (last 2 cells)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For T5 based model\n","#from transformers import InstructBlipConfig,InstructBlipForConditionalGeneration,InstructBlipProcessor\n","\n","#from transformers import InstructBlipModel, InstructBlipPreTrainedModel\n","import datasets\n","import transformers\n","from transformers import InstructBlipProcessor\n","\n","from PIL import Image\n","import torch\n","\n","model_type=\"instructblip\"\n","model_ckpt=\"BleachNick/MMICL-Instructblip-T5-xl\"\n","processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n","config = InstructBlipConfig.from_pretrained(model_ckpt)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","if 'instructblip' in model_type:\n","    model = InstructBlipForConditionalGeneration.from_pretrained(\n","        model_ckpt,\n","        config=config).to(device,dtype=torch.bfloat16)\n","\n","image_palceholder=\"å›¾\"\n","sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n","processor = InstructBlipProcessor.from_pretrained(\n","    processor_ckpt\n",")\n","sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n","processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n","if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n","    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n","replace_token=\"\".join(32*[image_palceholder])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Helper function for generating the responses to the ImageCoDe-simple task.\n","Note, you can modify the prompt by commenting out certain lines."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def MMICL(image0,image1,caption):\n","    im1 = Image.open(image0)\n","    im2 = Image.open(image1)\n","\n","    images = [im1,im2]\n","    prompt = [f'Use the image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aid to help you answer the questions accurately.',\n","              #f'Describe the differences and similarities of these images.',\n","              f'Tell me which image is best described by this caption: \"{caption}\".',\n","              \"End your answer with either: image2 or image1\",\n","              \"Justify your answer.\",\n","              \"\"]\n","    prompt = \" \".join(prompt)\n","\n","    inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n","\n","    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n","    inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n","    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n","\n","    inputs = inputs.to('cuda')\n","    outputs = model.generate(\n","            pixel_values = inputs['pixel_values'],\n","            input_ids = inputs['input_ids'],\n","            attention_mask = inputs['attention_mask'],\n","            img_mask = inputs['img_mask'],\n","            do_sample=False,\n","            max_length=50,\n","            min_length=1,\n","            num_beams=3,\n","            set_min_padding_size =False,\n","    )\n","    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n","    #print(generated_text)\n","    return generated_text\n"]},{"cell_type":"markdown","metadata":{},"source":["## Run the model to complete the task on the validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T06:14:04.764524Z","iopub.status.busy":"2024-04-23T06:14:04.763825Z","iopub.status.idle":"2024-04-23T06:30:31.301912Z","shell.execute_reply":"2024-04-23T06:30:31.300878Z","shell.execute_reply.started":"2024-04-23T06:14:04.764492Z"},"trusted":true},"outputs":[],"source":["image_generator = get_image_caption_pair()\n","\n","c = 0\n","results0 = []\n","results1 = []\n","\n","for pos_img,neg_img,caption in image_generator:\n","    if c >= 416:\n","        break\n","    \n","    ans0 = MMICL(pos_img,neg_img,caption)\n","    results0.append({\"img1\":pos_img, \"img2\":neg_img, \"caption\":caption, \"label\":\"image1\", \"answer\":ans0})\n","        \n","    ans1 = MMICL(neg_img,pos_img,caption)\n","    results1.append({\"img1\":neg_img, \"img2\":pos_img, \"caption\":caption, \"label\":\"image2\", \"answer\":ans1})"]},{"cell_type":"markdown","metadata":{},"source":["## Save the results in a json file."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T06:13:15.172490Z","iopub.status.busy":"2024-04-23T06:13:15.172139Z","iopub.status.idle":"2024-04-23T06:13:15.192551Z","shell.execute_reply":"2024-04-23T06:13:15.191674Z","shell.execute_reply.started":"2024-04-23T06:13:15.172463Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/working/BaseUnbalancedMMICLResults_image1_0-415Valid.json', 'w') as outfile:\n","    json.dump(results0, outfile, indent=4)\n","with open('/kaggle/working/BaseUnbalancedMMICLResults_image2_0-415Valid.json', 'w') as outfile:\n","    json.dump(results1, outfile, indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["### Code from the MMICL github\n","Source: https://github.com/HaozheZhao/MIC\n","\n","In order to run the notebook (i.e. the MMICL generate function), you must execute these last 2 cells first. \n","These are previous versions of helper functions for the InstructBLIP model, from which the MMICL model is based on. \n","If we used the most recent versions from the transformers library, the multi-image input would not be available. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# coding=utf-8\n","# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" InstructBLIP model configuration\"\"\"\n","\n","import copy\n","import os\n","from typing import Union\n","\n","from transformers.configuration_utils import PretrainedConfig\n","from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","from transformers.utils import logging\n","from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n","\n","\n","logger = logging.get_logger(__name__)\n","\n","INSTRUCTBLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n","    \"Salesforce/instruct-blip-flan-t5\": \"https://huggingface.co/Salesforce/instruct-blip-flan-t5/resolve/main/config.json\",\n","}\n","\n","\n","class InstructBlipVisionConfig(PretrainedConfig):\n","    r\"\"\"\n","    This is the configuration class to store the configuration of a [`InstructBlipVisionModel`]. It is used to\n","    instantiate a InstructBLIP vision encoder according to the specified arguments, defining the model architecture.\n","    Instantiating a configuration defaults will yield a similar configuration to that of the InstructBLIP\n","    [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n","\n","    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n","    documentation from [`PretrainedConfig`] for more information.\n","\n","    Args:\n","        hidden_size (`int`, *optional*, defaults to 1408):\n","            Dimensionality of the encoder layers and the pooler layer.\n","        intermediate_size (`int`, *optional*, defaults to 6144):\n","            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n","        num_hidden_layers (`int`, *optional*, defaults to 39):\n","            Number of hidden layers in the Transformer encoder.\n","        num_attention_heads (`int`, *optional*, defaults to 16):\n","            Number of attention heads for each attention layer in the Transformer encoder.\n","        image_size (`int`, *optional*, defaults to 224):\n","            The size (resolution) of each image.\n","        patch_size (`int`, *optional*, defaults to 14):\n","            The size (resolution) of each patch.\n","        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n","            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n","            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"gelu\"` are supported. layer_norm_eps (`float`, *optional*, defaults\n","            to 1e-5): The epsilon used by the layer normalization layers.\n","        dropout (`float`, *optional*, defaults to 0.0):\n","            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n","        attention_dropout (`float`, *optional*, defaults to 0.0):\n","            The dropout ratio for the attention probabilities.\n","        initializer_range (`float`, *optional*, defaults to 0.02):\n","            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n","        initializer_factor (`float``, *optional*, defaults to 1):\n","            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n","            testing).\n","        qkv_bias (`bool`, *optional*, defaults to `True`):\n","            Whether to add a bias to the queries and values in the self-attention layers.\n","\n","    Example:\n","\n","    ```python\n","    >>> from transformers import InstructBlipVisionConfig, InstructBlipVisionModel\n","\n","    >>> # Initializing a InstructBlipVisionConfig with Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> configuration = InstructBlipVisionConfig()\n","\n","    >>> # Initializing a InstructBlipVisionModel (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> model = InstructBlipVisionModel(configuration)\n","\n","    >>> # Accessing the model configuration\n","    >>> configuration = model.config\n","    ```\"\"\"\n","\n","    model_type = \"instructblip_vision_model\"\n","\n","    def __init__(\n","        self,\n","        hidden_size=1408,\n","        intermediate_size=6144,\n","        projection_dim=512,\n","        num_hidden_layers=39,\n","        num_attention_heads=16,\n","        num_channels=3,\n","        image_size=224,\n","        patch_size=14,\n","        hidden_act=\"gelu\",\n","        layer_norm_eps=0.00001,\n","        dropout=0.0,\n","        attention_dropout=0.0,\n","        initializer_range=1e-10,\n","        initializer_factor=1.0,\n","        qkv_bias=True,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.hidden_size = hidden_size\n","        self.intermediate_size = intermediate_size\n","        self.projection_dim = projection_dim\n","        self.dropout = dropout\n","        self.num_hidden_layers = num_hidden_layers\n","        self.num_attention_heads = num_attention_heads\n","        self.num_channels = num_channels\n","        self.patch_size = patch_size\n","        self.image_size = image_size\n","        self.initializer_range = initializer_range\n","        self.initializer_factor = initializer_factor\n","        self.attention_dropout = attention_dropout\n","        self.layer_norm_eps = layer_norm_eps\n","        self.hidden_act = hidden_act\n","        self.qkv_bias = qkv_bias\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n","        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n","\n","        # get the vision config dict if we are loading from InstructBlipConfig\n","        if config_dict.get(\"model_type\") == \"instructblip\":\n","            config_dict = config_dict[\"vision_config\"]\n","\n","        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n","            logger.warning(\n","                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n","                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n","            )\n","\n","        return cls.from_dict(config_dict, **kwargs)\n","\n","\n","class InstructBlipQFormerConfig(PretrainedConfig):\n","    r\"\"\"\n","    This is the configuration class to store the configuration of a [`InstructBlipQFormerModel`]. It is used to\n","    instantiate a InstructBLIP Querying Transformer (Q-Former) model according to the specified arguments, defining the\n","    model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n","    the InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)\n","    architecture. Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs.\n","    Read the documentation from [`PretrainedConfig`] for more information.\n","\n","    Note that [`InstructBlipQFormerModel`] is very similar to [`BertLMHeadModel`] with interleaved cross-attention.\n","\n","    Args:\n","        vocab_size (`int`, *optional*, defaults to 30522):\n","            Vocabulary size of the Q-Former model. Defines the number of different tokens that can be represented by\n","            the `inputs_ids` passed when calling the model.\n","        hidden_size (`int`, *optional*, defaults to 768):\n","            Dimensionality of the encoder layers and the pooler layer.\n","        num_hidden_layers (`int`, *optional*, defaults to 12):\n","            Number of hidden layers in the Transformer encoder.\n","        num_attention_heads (`int`, *optional*, defaults to 12):\n","            Number of attention heads for each attention layer in the Transformer encoder.\n","        intermediate_size (`int`, *optional*, defaults to 3072):\n","            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n","        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n","            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n","            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n","        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n","            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n","        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n","            The dropout ratio for the attention probabilities.\n","        max_position_embeddings (`int`, *optional*, defaults to 512):\n","            The maximum sequence length that this model might ever be used with. Typically set this to something large\n","            just in case (e.g., 512 or 1024 or 2048).\n","        initializer_range (`float`, *optional*, defaults to 0.02):\n","            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n","        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n","            The epsilon used by the layer normalization layers.\n","        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n","            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n","            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n","            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n","            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n","            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n","        classifier_dropout (`float`, *optional*):\n","            The dropout ratio for the classification head.\n","        cross_attention_frequency (`int`, *optional*, defaults to 2):\n","            The frequency of adding cross-attention to the Transformer layers.\n","        encoder_hidden_size (`int`, *optional*, defaults to 1408):\n","            The hidden size of the hidden states for cross-attention.\n","\n","    Examples:\n","\n","    ```python\n","    >>> from transformers import InstructBlipQFormerConfig, InstructBlipQFormerModel\n","\n","    >>> # Initializing a InstructBLIP Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> configuration = InstructBlipQFormerConfig()\n","\n","    >>> # Initializing a model (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> model = InstructBlipQFormerModel(configuration)\n","    >>> # Accessing the model configuration\n","    >>> configuration = model.config\n","    ```\"\"\"\n","    model_type = \"instructblip_qformer\"\n","\n","    def __init__(\n","        self,\n","        vocab_size=30522,\n","        hidden_size=768,\n","        num_hidden_layers=12,\n","        num_attention_heads=12,\n","        intermediate_size=3072,\n","        hidden_act=\"gelu\",\n","        hidden_dropout_prob=0.1,\n","        attention_probs_dropout_prob=0.1,\n","        max_position_embeddings=512,\n","        initializer_range=0.02,\n","        layer_norm_eps=1e-12,\n","        pad_token_id=0,\n","        position_embedding_type=\"absolute\",\n","        classifier_dropout=None,\n","        cross_attention_frequency=2,\n","        encoder_hidden_size=1408,\n","        **kwargs,\n","    ):\n","        super().__init__(pad_token_id=pad_token_id, **kwargs)\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_hidden_layers = num_hidden_layers\n","        self.num_attention_heads = num_attention_heads\n","        self.hidden_act = hidden_act\n","        self.intermediate_size = intermediate_size\n","        self.hidden_dropout_prob = hidden_dropout_prob\n","        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n","        self.max_position_embeddings = max_position_embeddings\n","        self.initializer_range = initializer_range\n","        self.layer_norm_eps = layer_norm_eps\n","        self.position_embedding_type = position_embedding_type\n","        self.classifier_dropout = classifier_dropout\n","        self.cross_attention_frequency = cross_attention_frequency\n","        self.encoder_hidden_size = encoder_hidden_size\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n","        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n","\n","        # get the qformer config dict if we are loading from InstructBlipConfig\n","        if config_dict.get(\"model_type\") == \"instructblip\":\n","            config_dict = config_dict[\"qformer_config\"]\n","\n","        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n","            logger.warning(\n","                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n","                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n","            )\n","\n","        return cls.from_dict(config_dict, **kwargs)\n","\n","\n","class InstructBlipConfig(PretrainedConfig):\n","    r\"\"\"\n","    [`InstructBlipConfig`] is the configuration class to store the configuration of a\n","    [`InstructBlipForConditionalGeneration`]. It is used to instantiate a InstructBLIP model according to the specified\n","    arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with\n","    the defaults will yield a similar configuration to that of the InstructBLIP\n","    [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n","\n","    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n","    documentation from [`PretrainedConfig`] for more information.\n","\n","    Args:\n","        vision_config (`dict`, *optional*):\n","            Dictionary of configuration options used to initialize [`InstructBlipVisionConfig`].\n","        qformer_config (`dict`, *optional*):\n","            Dictionary of configuration options used to initialize [`InstructBlipQFormerConfig`].\n","        text_config (`dict`, *optional*):\n","            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n","        num_query_tokens (`int`, *optional*, defaults to 32):\n","            The number of query tokens passed through the Transformer.\n","\n","        kwargs (*optional*):\n","            Dictionary of keyword arguments.\n","\n","    Example:\n","\n","    ```python\n","    >>> from transformers import (\n","    ...     InstructBlipVisionConfig,\n","    ...     InstructBlipQFormerConfig,\n","    ...     OPTConfig,\n","    ...     InstructBlipConfig,\n","    ...     InstructBlipForConditionalGeneration,\n","    ... )\n","\n","    >>> # Initializing a InstructBlipConfig with Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> configuration = InstructBlipConfig()\n","\n","    >>> # Initializing a InstructBlipForConditionalGeneration (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n","    >>> model = InstructBlipForConditionalGeneration(configuration)\n","\n","    >>> # Accessing the model configuration\n","    >>> configuration = model.config\n","\n","    >>> # We can also initialize a InstructBlipConfig from a InstructBlipVisionConfig, InstructBlipQFormerConfig and any PretrainedConfig\n","\n","    >>> # Initializing InstructBLIP vision, InstructBLIP Q-Former and language model configurations\n","    >>> vision_config = InstructBlipVisionConfig()\n","    >>> qformer_config = InstructBlipQFormerConfig()\n","    >>> text_config = OPTConfig()\n","\n","    >>> config = InstructBlipConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n","    ```\"\"\"\n","\n","    model_type = \"instructblip\"\n","    is_composition = True\n","\n","    def __init__(self, vision_config=None, qformer_config=None, text_config=None, num_query_tokens=32, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        if vision_config is None:\n","            vision_config = {}\n","            logger.info(\"vision_config is None. initializing the InstructBlipVisionConfig with default values.\")\n","\n","        if qformer_config is None:\n","            qformer_config = {}\n","            logger.info(\"qformer_config is None. Initializing the InstructBlipQFormerConfig with default values.\")\n","\n","        if text_config is None:\n","            text_config = {}\n","            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n","\n","        self.vision_config = InstructBlipVisionConfig(**vision_config)\n","        self.qformer_config = InstructBlipQFormerConfig(**qformer_config)\n","        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n","        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n","\n","        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n","        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n","\n","        self.num_query_tokens = num_query_tokens\n","        self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n","        self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n","        self.initializer_factor = 1.0\n","        self.initializer_range = 0.02\n","\n","    @classmethod\n","    def from_vision_qformer_text_configs(\n","        cls,\n","        vision_config: InstructBlipVisionConfig,\n","        qformer_config: InstructBlipQFormerConfig,\n","        text_config: PretrainedConfig,\n","        **kwargs,\n","    ):\n","        r\"\"\"\n","        Instantiate a [`InstructBlipConfig`] (or a derived class) from a InstructBLIP vision model, Q-Former and\n","        language model configurations.\n","\n","        Returns:\n","            [`InstructBlipConfig`]: An instance of a configuration object\n","        \"\"\"\n","\n","        return cls(\n","            vision_config=vision_config.to_dict(),\n","            qformer_config=qformer_config.to_dict(),\n","            text_config=text_config.to_dict(),\n","            **kwargs,\n","        )\n","\n","    def to_dict(self):\n","        \"\"\"\n","        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n","\n","        Returns:\n","            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n","        \"\"\"\n","        output = copy.deepcopy(self.__dict__)\n","        output[\"vision_config\"] = self.vision_config.to_dict()\n","        output[\"qformer_config\"] = self.qformer_config.to_dict()\n","        output[\"text_config\"] = self.text_config.to_dict()\n","        output[\"model_type\"] = self.__class__.model_type\n","        return output"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T03:48:07.672605Z","iopub.status.busy":"2024-04-23T03:48:07.672265Z","iopub.status.idle":"2024-04-23T03:48:08.496111Z","shell.execute_reply":"2024-04-23T03:48:08.495325Z","shell.execute_reply.started":"2024-04-23T03:48:07.672578Z"},"trusted":true},"outputs":[],"source":["# coding=utf-8\n","# Copyright 2023 The Salesforce Authors and The HuggingFace Team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" PyTorch InstructBLIP model.\"\"\"\n","\n","import math\n","from dataclasses import dataclass\n","from typing import Any, Optional, Tuple, Union\n","from collections import Counter\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","\n","from transformers.activations import ACT2FN\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    BaseModelOutputWithPastAndCrossAttentions,\n","    BaseModelOutputWithPooling,\n","    BaseModelOutputWithPoolingAndCrossAttentions,\n",")\n","from transformers.modeling_utils import PreTrainedModel\n","from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n","from transformers.utils import (\n","    ModelOutput,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    logging,\n","    replace_return_docstrings,\n",")\n","from transformers.models.auto.modeling_auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n","#from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n","\n","\n","logger = logging.get_logger(__name__)\n","\n","#_CHECKPOINT_FOR_DOC = \"Salesforce/instructblip-flan-t5\"\n","\n","INSTRUCTBLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"Salesforce/instructblip-flan-t5\",\n","    # See all InstructBLIP models at https://huggingface.co/models?filter=instructblip\n","]\n","\n","\n","@dataclass\n","# Copied from transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput with Blip2->InstructBlip\n","class InstructBlipForConditionalGenerationModelOutput(ModelOutput):\n","    \"\"\"\n","    Class defining the outputs of [`InstructBlipForConditionalGeneration`].\n","\n","    Args:\n","        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n","            Language modeling loss from the language model.\n","        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n","            Prediction scores of the language modeling head of the language model.\n","        vision_outputs (`BaseModelOutputWithPooling`):\n","            Outputs of the vision encoder.\n","        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n","            Outputs of the Q-Former (Querying Transformer).\n","        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n","            Outputs of the language model.\n","    \"\"\"\n","\n","    loss: Optional[Tuple[torch.FloatTensor]] = None\n","    logits: Optional[Tuple[torch.FloatTensor]] = None\n","    vision_outputs: Optional[torch.FloatTensor] = None\n","    qformer_outputs: Optional[Tuple[torch.FloatTensor]] = None\n","    language_model_outputs: Optional[Tuple[torch.FloatTensor]] = None\n","\n","    def to_tuple(self) -> Tuple[Any]:\n","        return tuple(\n","            self[k]\n","            if k not in [\"vision_outputs\", \"qformer_outputs\", \"language_model_outputs\"]\n","            else getattr(self, k).to_tuple()\n","            for k in self.keys()\n","        )\n","\n","\n","# Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->InstructBlip\n","class InstructBlipVisionEmbeddings(nn.Module):\n","    def __init__(self, config: InstructBlipVisionConfig):\n","        super().__init__()\n","        self.config = config\n","        self.embed_dim = config.hidden_size\n","        self.image_size = config.image_size\n","        self.patch_size = config.patch_size\n","\n","        self.class_embedding = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n","\n","        self.patch_embedding = nn.Conv2d(\n","            in_channels=3, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size\n","        )\n","\n","        self.num_patches = (self.image_size // self.patch_size) ** 2\n","        self.num_positions = self.num_patches + 1\n","\n","        self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n","\n","    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n","        batch_size = pixel_values.shape[0]\n","        target_dtype = self.patch_embedding.weight.dtype\n","        patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n","        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n","\n","        class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)\n","        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n","        embeddings = embeddings + self.position_embedding[:, : embeddings.size(1), :].to(target_dtype)\n","        return embeddings\n","\n","\n","# Copied from transformers.models.blip_2.modeling_blip_2.Blip2Attention with Blip2->InstructBlip\n","class InstructBlipAttention(nn.Module):\n","    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.embed_dim = config.hidden_size\n","        self.num_heads = config.num_attention_heads\n","        self.head_dim = self.embed_dim // self.num_heads\n","        if self.head_dim * self.num_heads != self.embed_dim:\n","            raise ValueError(\n","                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n","                f\" {self.num_heads}).\"\n","            )\n","        self.scale = self.head_dim**-0.5\n","        self.dropout = nn.Dropout(config.attention_dropout)\n","\n","        # small tweak here compared to CLIP, no bias here\n","        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n","\n","        if config.qkv_bias:\n","            q_bias = nn.Parameter(torch.zeros(self.embed_dim))\n","            v_bias = nn.Parameter(torch.zeros(self.embed_dim))\n","        else:\n","            q_bias = None\n","            v_bias = None\n","\n","        if q_bias is not None:\n","            qkv_bias = torch.cat((q_bias, torch.zeros_like(v_bias, requires_grad=False), v_bias))\n","            self.qkv.bias = nn.Parameter(qkv_bias)\n","\n","        self.projection = nn.Linear(self.embed_dim, self.embed_dim)\n","\n","    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n","        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        head_mask: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","        bsz, tgt_len, embed_dim = hidden_states.size()\n","\n","        mixed_qkv = self.qkv(hidden_states)\n","\n","        mixed_qkv = mixed_qkv.reshape(bsz, tgt_len, 3, self.num_heads, embed_dim // self.num_heads).permute(\n","            2, 0, 3, 1, 4\n","        )\n","        query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n","\n","        attention_scores = attention_scores * self.scale\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n","\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n","        context_layer = context_layer.reshape(new_context_layer_shape)\n","\n","        output = self.projection(context_layer)\n","\n","        outputs = (output, attention_probs) if output_attentions else (output, None)\n","\n","        return outputs\n","\n","\n","# Copied from transformers.models.blip.modeling_blip.BlipMLP\n","class InstructBlipMLP(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.activation_fn = ACT2FN[config.hidden_act]\n","        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.fc1(hidden_states)\n","        hidden_states = self.activation_fn(hidden_states)\n","        hidden_states = self.fc2(hidden_states)\n","        return hidden_states\n","\n","\n","# Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->InstructBlip\n","class InstructBlipEncoderLayer(nn.Module):\n","    def __init__(self, config: InstructBlipConfig):\n","        super().__init__()\n","        self.embed_dim = config.hidden_size\n","        self.self_attn = InstructBlipAttention(config)\n","        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n","        self.mlp = InstructBlipMLP(config)\n","        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.FloatTensor]:\n","        \"\"\"\n","        Args:\n","            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n","            attention_mask (`torch.FloatTensor`): attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","                `(config.encoder_attention_heads,)`.\n","            output_attentions (`bool`, *optional*):\n","                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","                returned tensors for more detail.\n","        \"\"\"\n","        residual = hidden_states\n","\n","        hidden_states = self.layer_norm1(hidden_states)\n","        hidden_states, attn_weights = self.self_attn(\n","            hidden_states=hidden_states,\n","            head_mask=attention_mask,\n","            output_attentions=output_attentions,\n","        )\n","        hidden_states = hidden_states + residual\n","        residual = hidden_states\n","        hidden_states = self.layer_norm2(hidden_states)\n","        hidden_states = self.mlp(hidden_states)\n","\n","        hidden_states = hidden_states + residual\n","\n","        outputs = (hidden_states,)\n","\n","        if output_attentions:\n","            outputs += (attn_weights,)\n","\n","        return outputs\n","\n","\n","class InstructBlipPreTrainedModel(PreTrainedModel):\n","    \"\"\"\n","    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n","    models.\n","    \"\"\"\n","\n","    config_class = InstructBlipConfig\n","    base_model_prefix = \"blip\"\n","    supports_gradient_checkpointing = True\n","    _keys_to_ignore_on_load_missing = [\n","        r\"position_ids\",\n","        r\"language_model.encoder.embed_tokens.weight\",\n","        r\"language_model.decoder.embed_tokens.weight\",\n","        r\"language_model.lm_head.weight\",\n","    ]\n","\n","\n","    # Copied from transformers.models.blip_2.modeling_blip_2.Blip2PreTrainedModel._init_weights with Blip2->InstructBlip\n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights\"\"\"\n","        factor = self.config.initializer_range\n","        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=factor)\n","            if hasattr(module, \"bias\") and module.bias is not None:\n","                module.bias.data.zero_()\n","\n","        if isinstance(module, InstructBlipVisionEmbeddings):\n","            if hasattr(self.config, \"vision_config\"):\n","                factor = self.config.vision_config.initializer_range\n","            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n","            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n","\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        elif isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def _set_gradient_checkpointing(self, module, value=False):\n","        if isinstance(module, InstructBlipEncoder):\n","            module.gradient_checkpointing = value\n","\n","\n","INSTRUCTBLIP_START_DOCSTRING = r\"\"\"\n","    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n","    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n","    etc.)\n","\n","    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n","    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n","    and behavior.\n","\n","    Parameters:\n","        config ([`InstructBlipConfig`]): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the\n","            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n","\"\"\"\n","\n","INSTRUCTBLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n","            Pixel values. Pixel values can be obtained using [`InstructBlipProcessor`]. See\n","            [`InstructBlipProcessor.__call__`] for details.\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","INSTRUCTBLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n","            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n","            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n","        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","            [What are attention masks?](../glossary#attention-mask)\n","        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","            Indices of decoder input sequence tokens in the vocabulary.\n","\n","            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details.\n","\n","            [What are decoder input IDs?](../glossary#decoder-input-ids)\n","\n","            T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n","            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n","\n","            To know more on how to prepare `decoder_input_ids` for pretraining take a look at [T5\n","            Training](./t5#training).\n","        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n","            be used by default.\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","INSTRUCTBLIP_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n","            Pixel values. Pixel values can be obtained using [`InstructBlipProcessor`]. See\n","            [`InstructBlipProcessor.__call__`] for details.\n","\n","        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n","            provided to serve as text prompt, which the language model can continue.\n","\n","            Indices can be obtained using [`InstructBlipProcessor`]. See [`InstructBlipProcessor.__call__`] for\n","            details.\n","\n","            [What are input IDs?](../glossary#input-ids)\n","        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","\n","            [What are attention masks?](../glossary#attention-mask)\n","\n","        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n","            encoder-decoder language model (like T5) is used.\n","\n","            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n","\n","        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n","            be used by default.\n","\n","            Only relevant in case an encoder-decoder language model (like T5) is used.\n","\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","\n","# Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip\n","class InstructBlipEncoder(nn.Module):\n","    \"\"\"\n","    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n","    [`InstructBlipEncoderLayer`].\n","\n","    Args:\n","        config (`InstructBlipConfig`):\n","            The corresponding vision configuration for the `InstructBlipEncoder`.\n","    \"\"\"\n","\n","    def __init__(self, config: InstructBlipConfig):\n","        super().__init__()\n","        self.config = config\n","        self.layers = nn.ModuleList([InstructBlipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n","        self.gradient_checkpointing = False\n","\n","    def forward(\n","        self,\n","        inputs_embeds,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutput]:\n","        r\"\"\"\n","        Args:\n","            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n","                Embedded representation of the inputs. Should be float, not int tokens.\n","            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                [What are attention masks?](../glossary#attention-mask)\n","            output_attentions (`bool`, *optional*):\n","                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","                returned tensors for more detail.\n","            output_hidden_states (`bool`, *optional*):\n","                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n","                for more detail.\n","            return_dict (`bool`, *optional*):\n","                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        encoder_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","\n","        hidden_states = inputs_embeds\n","        for idx, encoder_layer in enumerate(self.layers):\n","            if output_hidden_states:\n","                encoder_states = encoder_states + (hidden_states,)\n","            if self.gradient_checkpointing and self.training:\n","\n","                def create_custom_forward(module):\n","                    def custom_forward(*inputs):\n","                        return module(*inputs, output_attentions)\n","\n","                    return custom_forward\n","\n","                layer_outputs = torch.utils.checkpoint.checkpoint(\n","                    create_custom_forward(encoder_layer),\n","                    hidden_states,\n","                    attention_mask,\n","                )\n","            else:\n","                layer_outputs = encoder_layer(\n","                    hidden_states,\n","                    attention_mask,\n","                    output_attentions=output_attentions,\n","                )\n","\n","            hidden_states = layer_outputs[0]\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[1],)\n","\n","        if output_hidden_states:\n","            encoder_states = encoder_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n","        return BaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n","        )\n","\n","\n","# Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP\n","class InstructBlipVisionModel(InstructBlipPreTrainedModel):\n","    main_input_name = \"pixel_values\"\n","    config_class = InstructBlipVisionConfig\n","\n","    def __init__(self, config: InstructBlipVisionConfig):\n","        super().__init__(config)\n","        self.config = config\n","        embed_dim = config.hidden_size\n","\n","        self.embeddings = InstructBlipVisionEmbeddings(config)\n","        self.encoder = InstructBlipEncoder(config)\n","        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n","\n","        self.post_init()\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_VISION_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=InstructBlipVisionConfig)\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n","        r\"\"\"\n","        Returns:\n","\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if pixel_values is None:\n","            raise ValueError(\"You have to specify pixel_values\")\n","\n","        hidden_states = self.embeddings(pixel_values)\n","\n","        encoder_outputs = self.encoder(\n","            inputs_embeds=hidden_states,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        last_hidden_state = encoder_outputs[0]\n","        last_hidden_state = self.post_layernorm(last_hidden_state)\n","\n","        pooled_output = last_hidden_state[:, 0, :]\n","        pooled_output = self.post_layernorm(pooled_output)\n","\n","        if not return_dict:\n","            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPooling(\n","            last_hidden_state=last_hidden_state,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings\n","\n","\n","# Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerMultiHeadAttention with Blip2->InstructBlip\n","class InstructBlipQFormerMultiHeadAttention(nn.Module):\n","    def __init__(self, config, is_cross_attention=False):\n","        super().__init__()\n","        self.config = config\n","        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention heads (%d)\"\n","                % (config.hidden_size, config.num_attention_heads)\n","            )\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        if is_cross_attention:\n","            self.key = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n","            self.value = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n","        else:\n","            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n","        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n","            self.max_position_embeddings = config.max_position_embeddings\n","            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n","        self.save_attention = False\n","\n","    def save_attn_gradients(self, attn_gradients):\n","        self.attn_gradients = attn_gradients\n","\n","    def get_attn_gradients(self):\n","        return self.attn_gradients\n","\n","    def save_attention_map(self, attention_map):\n","        self.attention_map = attention_map\n","\n","    def get_attention_map(self):\n","        return self.attention_map\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_value=None,\n","        output_attentions=False,\n","    ):\n","        # If this is instantiated as a cross-attention module, the keys\n","        # and values come from an encoder; the attention mask needs to be\n","        # such that the encoder's padding tokens are not attended to.\n","        is_cross_attention = encoder_hidden_states is not None\n","\n","        if is_cross_attention:\n","            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n","            attention_mask = encoder_attention_mask\n","        elif past_key_value is not None:\n","            key_layer = self.transpose_for_scores(self.key(hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(hidden_states))\n","            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n","            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n","        else:\n","            key_layer = self.transpose_for_scores(self.key(hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(hidden_states))\n","\n","        mixed_query_layer = self.query(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","\n","        past_key_value = (key_layer, value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","\n","        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n","            seq_length = hidden_states.size()[1]\n","            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n","            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n","            distance = position_ids_l - position_ids_r\n","            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n","            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n","\n","            if self.position_embedding_type == \"relative_key\":\n","                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n","                attention_scores = attention_scores + relative_position_scores\n","            elif self.position_embedding_type == \"relative_key_query\":\n","                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n","                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n","                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n","\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","\n","        if attention_mask is not None:\n","            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        if is_cross_attention and self.save_attention:\n","            self.save_attention_map(attention_probs)\n","            attention_probs.register_hook(self.save_attn_gradients)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs_dropped = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs_dropped = attention_probs_dropped * head_mask\n","\n","        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n","\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","\n","        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        outputs = outputs + (past_key_value,)\n","        return outputs\n","\n","\n","# Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->InstructBlipQFormer\n","class InstructBlipQFormerSelfOutput(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","# Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerAttention with Blip2->InstructBlip\n","class InstructBlipQFormerAttention(nn.Module):\n","    def __init__(self, config, is_cross_attention=False):\n","        super().__init__()\n","        self.attention = InstructBlipQFormerMultiHeadAttention(config, is_cross_attention)\n","        self.output = InstructBlipQFormerSelfOutput(config)\n","        self.pruned_heads = set()\n","\n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        heads, index = find_pruneable_heads_and_indices(\n","            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n","        )\n","\n","        # Prune linear layers\n","        self.attention.query = prune_linear_layer(self.attention.query, index)\n","        self.attention.key = prune_linear_layer(self.attention.key, index)\n","        self.attention.value = prune_linear_layer(self.attention.value, index)\n","        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n","\n","        # Update hyper params and store pruned heads\n","        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n","        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n","        self.pruned_heads = self.pruned_heads.union(heads)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n","        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n","        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.Tensor]:\n","        self_outputs = self.attention(\n","            hidden_states,\n","            attention_mask,\n","            head_mask,\n","            encoder_hidden_states,\n","            encoder_attention_mask,\n","            past_key_value,\n","            output_attentions,\n","        )\n","        attention_output = self.output(self_outputs[0], hidden_states)\n","        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->InstructBlipQFormer\n","class InstructBlipQFormerIntermediate(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","        if isinstance(config.hidden_act, str):\n","            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","        return hidden_states\n","\n","\n","# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->InstructBlipQFormer\n","class InstructBlipQFormerOutput(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class InstructBlipQFormerLayer(nn.Module):\n","    def __init__(self, config, layer_idx):\n","        super().__init__()\n","        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n","        self.seq_len_dim = 1\n","        self.attention = InstructBlipQFormerAttention(config)\n","\n","        self.layer_idx = layer_idx\n","\n","        if layer_idx % config.cross_attention_frequency == 0:\n","            self.crossattention = InstructBlipQFormerAttention(config, is_cross_attention=True)\n","            self.has_cross_attention = True\n","        else:\n","            self.has_cross_attention = False\n","\n","        self.intermediate = InstructBlipQFormerIntermediate(config)\n","        self.output = InstructBlipQFormerOutput(config)\n","\n","        self.intermediate_query = InstructBlipQFormerIntermediate(config)\n","        self.output_query = InstructBlipQFormerOutput(config)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_value=None,\n","        output_attentions=False,\n","        query_length=0,\n","    ):\n","        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n","        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n","        self_attention_outputs = self.attention(\n","            hidden_states,\n","            attention_mask,\n","            head_mask,\n","            output_attentions=output_attentions,\n","            past_key_value=self_attn_past_key_value,\n","        )\n","        attention_output = self_attention_outputs[0]\n","        outputs = self_attention_outputs[1:-1]\n","\n","        present_key_value = self_attention_outputs[-1]\n","\n","        if query_length > 0:\n","            query_attention_output = attention_output[:, :query_length, :]\n","\n","            if self.has_cross_attention:\n","                if encoder_hidden_states is None:\n","                    raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n","                cross_attention_outputs = self.crossattention(\n","                    query_attention_output,\n","                    attention_mask,\n","                    head_mask,\n","                    encoder_hidden_states,\n","                    encoder_attention_mask,\n","                    output_attentions=output_attentions,\n","                )\n","                query_attention_output = cross_attention_outputs[0]\n","                # add cross attentions if we output attention weights\n","                outputs = outputs + cross_attention_outputs[1:-1]\n","\n","            layer_output = apply_chunking_to_forward(\n","                self.feed_forward_chunk_query,\n","                self.chunk_size_feed_forward,\n","                self.seq_len_dim,\n","                query_attention_output,\n","            )\n","\n","            if attention_output.shape[1] > query_length:\n","                layer_output_text = apply_chunking_to_forward(\n","                    self.feed_forward_chunk,\n","                    self.chunk_size_feed_forward,\n","                    self.seq_len_dim,\n","                    attention_output[:, query_length:, :],\n","                )\n","                layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n","        else:\n","            layer_output = apply_chunking_to_forward(\n","                self.feed_forward_chunk,\n","                self.chunk_size_feed_forward,\n","                self.seq_len_dim,\n","                attention_output,\n","            )\n","        outputs = (layer_output,) + outputs\n","\n","        outputs = outputs + (present_key_value,)\n","\n","        return outputs\n","\n","    def feed_forward_chunk(self, attention_output):\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.output(intermediate_output, attention_output)\n","        return layer_output\n","\n","    def feed_forward_chunk_query(self, attention_output):\n","        intermediate_output = self.intermediate_query(attention_output)\n","        layer_output = self.output_query(intermediate_output, attention_output)\n","        return layer_output\n","\n","\n","# Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerEncoder with Blip2->InstructBlip\n","class InstructBlipQFormerEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.layer = nn.ModuleList(\n","            [InstructBlipQFormerLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n","        )\n","        self.gradient_checkpointing = False\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=False,\n","        output_hidden_states=False,\n","        return_dict=True,\n","        query_length=0,\n","    ):\n","        all_hidden_states = () if output_hidden_states else None\n","        all_self_attentions = () if output_attentions else None\n","        all_cross_attentions = () if output_attentions else None\n","\n","        next_decoder_cache = () if use_cache else None\n","\n","        for i in range(self.config.num_hidden_layers):\n","            layer_module = self.layer[i]\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_head_mask = head_mask[i] if head_mask is not None else None\n","            past_key_value = past_key_values[i] if past_key_values is not None else None\n","\n","            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n","                if use_cache:\n","                    logger.warn(\n","                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n","                    )\n","                    use_cache = False\n","\n","                def create_custom_forward(module):\n","                    def custom_forward(*inputs):\n","                        return module(*inputs, past_key_value, output_attentions, query_length)\n","\n","                    return custom_forward\n","\n","                layer_outputs = torch.utils.checkpoint.checkpoint(\n","                    create_custom_forward(layer_module),\n","                    hidden_states,\n","                    attention_mask,\n","                    layer_head_mask,\n","                    encoder_hidden_states,\n","                    encoder_attention_mask,\n","                )\n","            else:\n","                layer_outputs = layer_module(\n","                    hidden_states,\n","                    attention_mask,\n","                    layer_head_mask,\n","                    encoder_hidden_states,\n","                    encoder_attention_mask,\n","                    past_key_value,\n","                    output_attentions,\n","                    query_length,\n","                )\n","\n","            hidden_states = layer_outputs[0]\n","            if use_cache:\n","                next_decoder_cache += (layer_outputs[-1],)\n","            if output_attentions:\n","                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n","                if layer_module.has_cross_attention:\n","                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n","\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(\n","                v\n","                for v in [\n","                    hidden_states,\n","                    next_decoder_cache,\n","                    all_hidden_states,\n","                    all_self_attentions,\n","                    all_cross_attentions,\n","                ]\n","                if v is not None\n","            )\n","        return BaseModelOutputWithPastAndCrossAttentions(\n","            last_hidden_state=hidden_states,\n","            past_key_values=next_decoder_cache,\n","            hidden_states=all_hidden_states,\n","            attentions=all_self_attentions,\n","            cross_attentions=all_cross_attentions,\n","        )\n","\n","\n","class InstructBlipQFormerEmbeddings(nn.Module):\n","    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","\n","        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n","        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n","        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n","\n","        self.config = config\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        position_ids=None,\n","        query_embeds=None,\n","        past_key_values_length=0,\n","    ):\n","        if input_ids is not None:\n","            seq_length = input_ids.size()[1]\n","        else:\n","            seq_length = 0\n","\n","        if position_ids is None:\n","            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length].clone()\n","\n","        if input_ids is not None:\n","            embeddings = self.word_embeddings(input_ids)\n","            if self.position_embedding_type == \"absolute\":\n","                position_embeddings = self.position_embeddings(position_ids)\n","                embeddings = embeddings + position_embeddings\n","\n","            if query_embeds is not None:\n","                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n","        else:\n","            embeddings = query_embeds\n","\n","        embeddings = self.layernorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class InstructBlipQFormerModel(InstructBlipPreTrainedModel):\n","    \"\"\"\n","    Querying Transformer (Q-Former), used in InstructBLIP. Slightly modified from BLIP-2 as it also takes the\n","    instruction as input.\n","    \"\"\"\n","\n","    def __init__(self, config: InstructBlipQFormerConfig):\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.embeddings = InstructBlipQFormerEmbeddings(config)\n","\n","        self.encoder = InstructBlipQFormerEncoder(config)\n","\n","        self.post_init()\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings.word_embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def get_extended_attention_mask(\n","        self,\n","        attention_mask: torch.Tensor,\n","        input_shape: Tuple[int],\n","        device: torch.device,\n","        has_query: bool = False,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n","\n","        Arguments:\n","            attention_mask (`torch.Tensor`):\n","                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n","            input_shape (`Tuple[int]`):\n","                The shape of the input to the model.\n","            device: (`torch.device`):\n","                The device of the input to the model.\n","\n","        Returns:\n","            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n","        \"\"\"\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        if attention_mask.dim() == 3:\n","            extended_attention_mask = attention_mask[:, None, :, :]\n","        elif attention_mask.dim() == 2:\n","            # Provided a padding mask of dimensions [batch_size, seq_length]\n","            # - the model is an encoder, so make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","            extended_attention_mask = attention_mask[:, None, None, :]\n","        else:\n","            raise ValueError(\n","                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n","                    input_shape, attention_mask.shape\n","                )\n","            )\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","        return extended_attention_mask\n","\n","    def forward(\n","        self,\n","        input_ids =None,\n","        attention_mask=None,\n","        position_ids=None,\n","        query_embeds=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n","            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n","            the model is configured as a decoder.\n","        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n","            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n","            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n","            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n","            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n","            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n","            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n","            `(batch_size, sequence_length)`.\n","        use_cache (`bool`, `optional`):\n","            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n","            `past_key_values`).\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if input_ids is None:\n","            if query_embeds is None:\n","                raise ValueError(\"You have to specify query_embeds when input_ids is None\")\n","\n","        # past_key_values_length\n","        past_key_values_length = (\n","            past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n","        )\n","\n","        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n","\n","        embedding_output = self.embeddings(\n","            input_ids=input_ids,\n","            position_ids=position_ids,\n","            query_embeds=query_embeds,\n","            past_key_values_length=past_key_values_length,\n","        )\n","\n","        input_shape = embedding_output.size()[:-1]\n","        batch_size, seq_length = input_shape\n","        device = embedding_output.device\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n","\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device)\n","\n","        # If a 2D or 3D attention mask is provided for the cross-attention\n","        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","        if encoder_hidden_states is not None:\n","            if type(encoder_hidden_states) == list:\n","                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n","            else:\n","                (\n","                    encoder_batch_size,\n","                    encoder_sequence_length,\n","                    _,\n","                ) = encoder_hidden_states.size()\n","            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n","\n","            if type(encoder_attention_mask) == list:\n","                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n","            elif encoder_attention_mask is None:\n","                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n","                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","            else:\n","                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","        else:\n","            encoder_extended_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            attention_mask=extended_attention_mask,\n","            head_mask=head_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_extended_attention_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            query_length=query_length,\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = sequence_output[:, 0, :]\n","\n","        if not return_dict:\n","            return (sequence_output, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPoolingAndCrossAttentions(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            past_key_values=encoder_outputs.past_key_values,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","            cross_attentions=encoder_outputs.cross_attentions,\n","        )\n","\n","\n","@add_start_docstrings(\n","    \"\"\"\n","    InstructBLIP Model for generating text and image features. The model consists of a vision encoder, Querying\n","    Transformer (Q-Former) and a language model.\n","    \"\"\",\n","    INSTRUCTBLIP_START_DOCSTRING,\n",")\n","class InstructBlipModel(InstructBlipPreTrainedModel):\n","    config_class = InstructBlipConfig\n","    main_input_name = \"pixel_values\"\n","\n","    # Copied from transformers.models.blip_2.modeling_blip_2.Blip2Model.__init__ with Blip2->InstructBlip,BLIP_2->INSTRUCTBLIP,Salesforce/blip2-opt-2.7b->Salesforce/instructblip-flan-t5\n","    def __init__(self, config: InstructBlipConfig):\n","        super().__init__(config)\n","\n","        self.vision_model = InstructBlipVisionModel(config.vision_config)\n","\n","        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n","        self.qformer = InstructBlipQFormerModel(config.qformer_config)\n","\n","        self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n","        if config.use_decoder_only_language_model:\n","            language_model = AutoModelForCausalLM.from_config(config.text_config)\n","        else:\n","            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n","        self.language_model = language_model\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def get_input_embeddings(self):\n","        return self.language_model.get_input_embeddings()\n","\n","    def set_input_embeddings(self, value):\n","        self.language_model.set_input_embeddings(value)\n","\n","    def set_output_embeddings(self, new_embeddings):\n","        self.language_model.set_output_embeddings(new_embeddings)\n","\n","    def get_output_embeddings(self) -> nn.Module:\n","        return self.language_model.get_output_embeddings()\n","\n","    def get_encoder(self):\n","        return self.language_model.get_encoder()\n","\n","    def get_decoder(self):\n","        return self.language_model.get_decoder()\n","\n","    def _tie_weights(self):\n","        if not self.config.use_decoder_only_language_model:\n","            self.language_model.encoder.embed_tokens = self.language_model.shared\n","            self.language_model.decoder.embed_tokens = self.language_model.shared\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_TEXT_INPUTS_DOCSTRING)\n","    def get_text_features(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        decoder_input_ids: Optional[torch.Tensor] = None,\n","        decoder_attention_mask: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ):\n","        r\"\"\"\n","        Returns:\n","            text_outputs (`CausalLMOutputWithPast`, or `tuple(torch.FloatTensor)` if `return_dict=False`):\n","                The language model outputs. If `return_dict=True`, the output is a [`CausalLMOutputWithPast`] that\n","                contains the language model logits, the past key values and the hidden states if\n","                `output_hidden_states=True`.\n","\n","        Examples:\n","\n","        ```python\n","        >>> import torch\n","        >>> from transformers import AutoTokenizer, InstructBlipModel\n","\n","        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        >>> model = InstructBlipModel.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","\n","        >>> model.to(device)  # doctest: +IGNORE_RESULT\n","\n","        >>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\").to(device)\n","        >>> text_features = model.get_text_features(**inputs)\n","        ```\"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if self.config.use_decoder_only_language_model:\n","            text_outputs = self.language_model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","        else:\n","            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n","\n","            text_outputs = self.language_model(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","                labels=labels,\n","            )\n","\n","        return text_outputs\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_VISION_INPUTS_DOCSTRING)\n","    def get_image_features(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ):\n","        r\"\"\"\n","        Returns:\n","            vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`):\n","                The vision model outputs. If `return_dict=True`, the output is a [`BaseModelOutputWithPooling`] that\n","                contains the image features, the pooled image features and the hidden states if\n","                `output_hidden_states=True`.\n","\n","        Examples:\n","        ```python\n","        >>> import torch\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import AutoProcessor, InstructBlipModel\n","\n","        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        >>> model = InstructBlipModel.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","\n","        >>> model.to(device)  # doctest: +IGNORE_RESULT\n","\n","        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","        >>> inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","        >>> image_outputs = model.get_image_features(**inputs)\n","        ```\"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        return vision_outputs\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n","    def get_qformer_features(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        qformer_input_ids: torch.FloatTensor = None,\n","        qformer_attention_mask: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ):\n","        r\"\"\"\n","        Returns:\n","            vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`):\n","                The vision model outputs. If `return_dict=True`, the output is a [`BaseModelOutputWithPooling`] that\n","                contains the image features, the pooled image features and the hidden states if\n","                `output_hidden_states=True`.\n","\n","        Examples:\n","\n","        ```python\n","        >>> import torch\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import InstructBlipProcessor, InstructBlipModel\n","\n","        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        >>> processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> model = InstructBlipModel.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> model.to(device)  # doctest: +IGNORE_RESULT\n","\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","        >>> inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","        >>> qformer_outputs = model.get_qformer_features(**inputs)\n","        ```\"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        image_embeds = vision_outputs[0]\n","\n","        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n","        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","\n","        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n","        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n","        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","        if qformer_attention_mask is None:\n","            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n","        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n","        query_outputs = self.qformer(\n","            input_ids=qformer_input_ids,\n","            attention_mask=qformer_attention_mask,\n","            query_embeds=query_tokens,\n","            encoder_hidden_states=image_embeds,\n","            encoder_attention_mask=image_attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        return query_outputs\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(\n","        output_type=InstructBlipForConditionalGenerationModelOutput, config_class=InstructBlipVisionConfig\n","    )\n","    def forward(\n","        self,\n","        pixel_values: torch.FloatTensor,\n","        qformer_input_ids: torch.FloatTensor,\n","        qformer_attention_mask: Optional[torch.LongTensor] = None,\n","        input_ids: Optional[torch.FloatTensor] = None,\n","        attention_mask: Optional[torch.LongTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import InstructBlipProcessor, InstructBlipModel\n","        >>> import torch\n","\n","        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        >>> processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> model = InstructBlipModel.from_pretrained(\"Salesforce/instructblip-flan-t5\")\n","        >>> model.to(device)  # doctest: +IGNORE_RESULT\n","\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","\n","        >>> prompt = \"Question: how many cats are there? Answer:\"\n","        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n","\n","        >>> outputs = model(**inputs)\n","        ```\"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # step 1: forward the images through the vision encoder,\n","        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        image_embeds = vision_outputs[0]\n","\n","        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n","        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","\n","        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n","        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n","        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","        if qformer_attention_mask is None:\n","            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n","        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n","        query_outputs = self.qformer(\n","            input_ids=qformer_input_ids,\n","            attention_mask=qformer_attention_mask,\n","            query_embeds=query_tokens,\n","            encoder_hidden_states=image_embeds,\n","            encoder_attention_mask=image_attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n","\n","        # step 3: use the language model, conditioned on the query outputs and the prompt\n","        language_model_inputs = self.language_projection(query_output)\n","        language_model_attention_mask = torch.ones(\n","            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n","        )\n","        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n","        inputs_embeds = torch.cat([language_model_inputs, inputs_embeds], dim=1)\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        expected_device = language_model_attention_mask.device\n","        attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n","\n","        if self.config.use_decoder_only_language_model:\n","            outputs = self.language_model(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            logits = outputs.logits if return_dict else outputs[0]\n","            loss = None\n","            # we compute the loss here since we need to take into account the sequence length of the query embeds\n","            if labels is not None:\n","                labels = labels.to(logits.device)\n","                logits = logits[:, -labels.size(1) :, :]\n","                # Shift so that tokens < n predict n\n","                shift_logits = logits[..., :-1, :].contiguous()\n","                shift_labels = labels[..., 1:].contiguous().to(logits.device)\n","\n","                # Flatten the tokens\n","                loss_fct = CrossEntropyLoss(reduction=\"mean\")\n","\n","                loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n","        else:\n","            outputs = self.language_model(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","                labels=labels,\n","            )\n","            loss = outputs.loss if return_dict else outputs[0]\n","            logits = outputs.logits if return_dict else outputs[1]\n","\n","        if not return_dict:\n","            output = (logits, vision_outputs, query_outputs, outputs)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return InstructBlipForConditionalGenerationModelOutput(\n","            loss=loss,\n","            logits=logits,\n","            vision_outputs=vision_outputs,\n","            qformer_outputs=query_outputs,\n","            language_model_outputs=outputs,\n","        )\n","\n","\n","@add_start_docstrings(\n","    \"\"\"\n","    InstructBLIP Model for generating text given an image and an optional text prompt. The model consists of a vision\n","    encoder, Querying Transformer (Q-Former) and a language model.\n","\n","    One can optionally pass `input_ids` to the model, which serve as a text prompt, to make the language model continue\n","    the prompt. Otherwise, the language model starts generating text from the [BOS] (beginning-of-sequence) token.\n","    \"\"\",\n","    INSTRUCTBLIP_START_DOCSTRING,\n",")\n","class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel):\n","    config_class = InstructBlipConfig\n","    main_input_name = \"pixel_values\"\n","\n","    def __init__(self, config: InstructBlipConfig):\n","        super().__init__(config)\n","\n","        self.vision_model = InstructBlipVisionModel(config.vision_config)\n","\n","        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n","        self.qformer = InstructBlipQFormerModel(config.qformer_config)\n","\n","        self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n","        if config.use_decoder_only_language_model:\n","            language_model = AutoModelForCausalLM.from_config(config.text_config)\n","        else:\n","            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n","\n","        self.language_model = language_model\n","        \n","        self.max_sequence_length = config.text_config.max_sequence_length if hasattr(config.text_config, \"max_sequence_length\") else 512\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def get_input_embeddings(self):\n","        return self.language_model.get_input_embeddings()\n","\n","    def set_input_embeddings(self, value):\n","        self.language_model.set_input_embeddings(value)\n","\n","    def set_output_embeddings(self, new_embeddings):\n","        self.language_model.set_output_embeddings(new_embeddings)\n","\n","    def get_output_embeddings(self) -> nn.Module:\n","        return self.language_model.get_output_embeddings()\n","\n","    def get_encoder(self):\n","        return self.language_model.get_encoder()\n","\n","    def get_decoder(self):\n","        return self.language_model.get_decoder()\n","\n","    def _tie_weights(self):\n","        if not self.config.use_decoder_only_language_model:\n","            self.language_model.encoder.embed_tokens = self.language_model.shared\n","            self.language_model.decoder.embed_tokens = self.language_model.shared\n","\n","    def _preprocess_accelerate(self):\n","        r\"\"\"\n","        Some pre-processing hacks to make the model `accelerate` compatible. Check\n","        https://github.com/huggingface/transformers/pull/21707 for more details.\n","        \"\"\"\n","        hf_device_map = self.hf_device_map\n","\n","        if len(hf_device_map) > 1 and \"language_model\" not in hf_device_map and torch.cuda.device_count() > 1:\n","            # warn users about unexpected behavior when using multi-GPU + InstructBLIP + `accelerate`.\n","            logger.warning(\n","                \"The `language_model` is not in the `hf_device_map` dictionary and you are running your script\"\n","                \" in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\"\n","                \" Please pass a `device_map` that contains `language_model` to remove this warning.\"\n","                \" Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for\"\n","                \" more details on creating a `device_map` for large models.\",\n","            )\n","\n","        if hasattr(self.language_model, \"_hf_hook\"):\n","            self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n","\n","    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(\n","        output_type=InstructBlipForConditionalGenerationModelOutput, config_class=InstructBlipVisionConfig\n","    )\n","    def forward(\n","        self,\n","        pixel_values: torch.FloatTensor= None,\n","        qformer_input_ids: torch.FloatTensor = None,\n","        qformer_attention_mask: Optional[torch.LongTensor] = None,\n","        input_ids: Optional[torch.FloatTensor] = None,\n","        attention_mask: Optional[torch.LongTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.LongTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        return_dict: Optional[bool] = None,\n","        img_mask: Optional[torch.Tensor] = None,\n","        set_min_padding_size: bool =True,\n","        sp_token: Optional[int] = 32100, # flant5-- 32100; vicuna-- 32001\n","        **kwargs,\n","    ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n","        >>> import torch\n","        >>> from PIL import Image\n","        >>> import requests\n","\n","        >>> model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n","        >>> processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n","\n","        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        >>> model.to(device)\n","\n","        >>> url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n","        >>> prompt = \"What is unusual about this image?\"\n","        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n","\n","        >>> outputs = model.generate(\n","        ...     **inputs,\n","        ...     do_sample=False,\n","        ...     num_beams=5,\n","        ...     max_length=256,\n","        ...     min_length=1,\n","        ...     top_p=0.9,\n","        ...     repetition_penalty=1.5,\n","        ...     length_penalty=1.0,\n","        ...     temperature=1,\n","        ... )\n","        >>> generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n","        >>> print(generated_text)\n","        What is unusual about this image? The image is unusual because it depicts a person standing on top of a car, which is parked on the side of the road. This is an unusual position for a person to be in, as they are typically not expected to stand on top of a car while it is parked. Additionally, the person in the image appears to be wearing a suit and tie, which is not typical attire for someone who is standing on top of a car. It is unclear why the person is in this unusual position or what they are doing there.\n","        ```\"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        batch_size = input_ids.shape[0]\n","        # step 1: forward the images through the vision encoder,\n","        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n","        if pixel_values is None:\n","            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)    \n","            if attention_mask is None:\n","                attention_mask = torch.ones_like(input_ids)\n","            attention_mask = attention_mask.to(inputs_embeds.device)\n","        else:\n","            if img_mask is None:\n","                img_mask = torch.ones(pixel_values.shape[:2])\n","\n","            pixel_values = pixel_values[img_mask.bool()]\n","            img_mask = img_mask.bool()\n","            img_count = img_mask.sum(1)\n","            vision_outputs = self.vision_model(\n","                pixel_values=pixel_values,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            image_embeds = vision_outputs[0]\n","            # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n","            \n","            image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","            # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n","\n","            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n","\n","\n","            if qformer_input_ids is not None:\n","\n","                query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","\n","                if qformer_attention_mask is None:\n","                    qformer_attention_mask = torch.ones_like(qformer_input_ids,device =image_embeds.device )\n","                qformer_ids =[]\n","                qformer_mask = []\n","                for index in range(batch_size):\n","                    q_ids = qformer_input_ids[index].unsqueeze(0).repeat(img_count[index],1)\n","                    q_mask = qformer_attention_mask[index].unsqueeze(0).repeat(img_count[index],1)\n","                    qformer_ids.append(q_ids)\n","                    qformer_mask.append(q_mask)\n","                qformer_ids = torch.concat(qformer_ids,0)\n","                qformer_mask = torch.concat(qformer_mask,0)\n","                qformer_mask = torch.cat([query_attention_mask, qformer_mask], dim=1)\n","                query_outputs = self.qformer(\n","                    input_ids=qformer_ids,\n","                    attention_mask=qformer_mask,\n","                    query_embeds=query_tokens,\n","                    encoder_hidden_states=image_embeds,\n","                    encoder_attention_mask=image_attention_mask,\n","                    return_dict=True,\n","                )\n","                query_output = query_outputs.last_hidden_state[:, : query_tokens.size(1), :]\n","            else:\n","                query_outputs = self.qformer(\n","                        query_embeds=query_tokens,\n","                        encoder_hidden_states=image_embeds,\n","                        encoder_attention_mask=image_attention_mask,\n","                        output_attentions=output_attentions,\n","                        output_hidden_states=output_hidden_states,\n","                        return_dict=return_dict,\n","                    )\n","                query_output = query_outputs[0]\n","            # step 3: use the language model, conditioned on the query outputs and the prompt\n","\n","            language_model_inputs = self.language_projection(query_output)\n","\n","            if attention_mask is None:\n","                attention_mask = torch.ones_like(input_ids)\n","                attention_mask = attention_mask.to(language_model_inputs.device)\n","\n","            inputs_embeds = self.get_input_embeddings()(input_ids)\n","            image_embeds_index = torch.where(input_ids == sp_token)\n","\n","            replace_length = inputs_embeds[image_embeds_index].shape[0]\n","        \n","            insert_embeds = language_model_inputs.reshape(-1,language_model_inputs.shape[-1])\n","\n","            insert_length = insert_embeds.shape[0]\n","        \n","            if insert_embeds.dtype != inputs_embeds.dtype:\n","                insert_embeds = insert_embeds.to(inputs_embeds.dtype)\n","\n","            if replace_length == insert_length:\n","                inputs_embeds[image_embeds_index] = insert_embeds\n","\n","            elif insert_length> replace_length:\n","                print(f\"shape mismatch leads to truncate. insert embedding tensor of shape {insert_embeds.shape} cannot be broadcast to replace placeholder of shape {inputs_embeds[image_embeds_index].shape}\")\n","                counts = Counter(image_embeds_index[0].cpu().detach().numpy())\n","                img_token_szie = 32\n","                # Sort the counts in ascending order of the numbers\n","                sorted_counts = sorted(counts.items())\n","\n","                # Extract the counts as a list\n","                result = [ (number,count) for number, count in sorted_counts]\n","                index = 0\n","                insert_embeds_list= []\n","\n","                img_idx = 0\n","                for num,each in result:\n","                    i_count = img_count[img_idx]\n","                    if img_idx != num: # skip sp_token\n","                        index+=i_count*img_token_szie\n","                        img_idx+=1\n","                        continue\n","                    if int(each / img_token_szie) == i_count:\n","                        insert_embeds_list.append(insert_embeds[index:index+each])\n","                        index+=each\n","                    else:\n","                        insert_embeds_list.append(insert_embeds[index:index+each])\n","                        index+= i_count*img_token_szie\n","                    img_idx +=1\n","                        \n","                new_insert_embeds = torch.concat(insert_embeds_list, dim=0)\n","                try:\n","                    inputs_embeds[image_embeds_index] = new_insert_embeds\n","                except:\n","                    print(f\"shape mismatch leads to HARD truncate. Require to fix the code! Insert embedding tensor of shape {insert_embeds.shape} cannot be broadcast to replace placeholder of shape {inputs_embeds[image_embeds_index].shape}\")\n","                    inputs_embeds[image_embeds_index] = insert_embeds[:replace_length]\n","\n","                        \n","\n","            # inputs_embeds[image_embeds_index] = language_model_inputs.reshape(-1,language_model_inputs.shape[-1])\n","\n","\n","\n","        min_padding_size = min(input_ids.shape[-1],self.max_sequence_length)\n","\n","        if not set_min_padding_size:\n","            min_padding_size = 10000 \n","        if self.config.use_decoder_only_language_model:\n","        # vicuna\n","            inputs_embeds = inputs_embeds[:,-min_padding_size:]\n","            attention_mask = attention_mask[:,-min_padding_size:]\n","        # t5\n","        else:\n","            inputs_embeds = inputs_embeds[:,:min_padding_size]\n","            attention_mask = attention_mask[:,:min_padding_size]\n","\n","        if self.config.use_decoder_only_language_model:\n","            outputs = self.language_model(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            logits = outputs.logits if return_dict else outputs[0]\n","            loss = None\n","            # we compute the loss here since we need to take into account the sequence length of the query embeds\n","            if labels is not None:\n","                # vicuna\n","                # labels = labels[:,-min_padding_size:]\n","                # t5\n","                labels = labels[:,:min_padding_size]\n","                labels = labels.to(logits.device)\n","                logits = logits[:, -labels.size(1) :, :]\n","                # Shift so that tokens < n predict n\n","                shift_logits = logits[..., :-1, :].contiguous()\n","                shift_labels = labels[..., 1:].contiguous().to(logits.device)\n","\n","                # Flatten the tokens\n","                loss_fct = CrossEntropyLoss(reduction=\"mean\")\n","\n","                loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n","        else:\n","            outputs = self.language_model(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","                labels=labels,\n","            )\n","            loss = outputs.loss if return_dict else outputs[0]\n","            logits = outputs.logits if return_dict else outputs[1]\n","\n","        if not return_dict:\n","            output = (logits, vision_outputs, query_outputs, outputs)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return InstructBlipForConditionalGenerationModelOutput(\n","            loss=loss,\n","            logits=logits,\n","            vision_outputs=vision_outputs,\n","            qformer_outputs=query_outputs,\n","            language_model_outputs=outputs,\n","        )\n","\n","    @torch.no_grad()\n","    def generate(\n","        self,\n","        pixel_values: torch.FloatTensor = None,\n","        qformer_input_ids: Optional[torch.LongTensor] = None,\n","        qformer_attention_mask: Optional[torch.LongTensor] = None,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.LongTensor] = None,\n","        img_mask: Optional[torch.Tensor] = None,\n","        set_min_padding_size: bool = True ,\n","        sp_token: Optional[int] = 32100, # flant5-- 32100; vicuna-- 32001\n","        **generate_kwargs,\n","    ) -> torch.LongTensor:\n","        \"\"\"\n","        Overrides `generate` function to be able to use the model as a conditional generator.\n","\n","        Args:\n","            pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width)):\n","                Input images to be processed.\n","            qformer_input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n","                The sequence used as a prompt to be fed to the Q-Former module.\n","            qformer_attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n","                Mask to avoid performing attention on padding token indices.\n","            input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n","                The sequence used as a prompt for the generation.\n","            attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n","                Mask to avoid performing attention on padding token indices.\n","\n","        Returns:\n","            captions (list): A list of strings of length batch_size * num_captions.\n","        \"\"\"\n","        if hasattr(self, \"hf_device_map\"):\n","            # preprocess for `accelerate`\n","            self._preprocess_accelerate()\n","\n","        caption_flag = False\n","        batch_size = input_ids.shape[0]\n","        if input_ids is None:\n","            input_ids = (\n","                torch.LongTensor([[self.config.text_config.bos_token_id]])\n","                .repeat(batch_size, 1)\n","                .to(self.device)\n","            )\n","            if  qformer_input_ids is None:\n","                qformer_input_ids = (\n","                    torch.LongTensor([[101, 102]]) # [CLS][SEP]\n","                    .repeat(batch_size, 1)\n","                    .to(self.device)\n","                )\n","            caption_flag = True\n","        if pixel_values is None:\n","            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)    \n","            if attention_mask is None:\n","                attention_mask = torch.ones_like(input_ids)\n","            attention_mask = attention_mask.to(inputs_embeds.device)\n","        else:\n","            if img_mask is None:\n","                img_mask = torch.ones(pixel_values.shape[:2])\n","            img_count = img_mask.sum(1)\n","            pixel_values = pixel_values[img_mask.bool()]\n","            \n","            image_embeds = self.vision_model(pixel_values, return_dict=True).last_hidden_state\n","\n","            \n","            image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","\n","            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n","                \n","            query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n","            if qformer_input_ids is not None:\n","\n","                if qformer_attention_mask is None:\n","                    qformer_attention_mask = torch.ones_like(qformer_input_ids,device =image_embeds.device )\n","                qformer_ids =[]\n","                qformer_mask = []\n","                for index in range(batch_size):\n","                    q_ids = qformer_input_ids[index].unsqueeze(0).repeat(img_count[index],1) if img_count[index]!=1 else qformer_input_ids[index].unsqueeze(0)\n","                    q_mask = qformer_attention_mask[index].unsqueeze(0).repeat(img_count[index],1) if img_count[index]!=1 else qformer_attention_mask[index].unsqueeze(0)\n","                    qformer_ids.append(q_ids)\n","                    qformer_mask.append(q_mask)\n","                qformer_ids = torch.concat(qformer_ids,0)\n","                qformer_mask = torch.concat(qformer_mask,0)\n","                qformer_mask = torch.cat([query_attention_mask, qformer_mask], dim=1)\n","                query_outputs = self.qformer(\n","                    input_ids=qformer_ids,\n","                    attention_mask=qformer_mask,\n","                    query_embeds=query_tokens,\n","                    encoder_hidden_states=image_embeds,\n","                    encoder_attention_mask=image_attention_mask,\n","                    return_dict=True,\n","                )\n","                query_output = query_outputs.last_hidden_state[:, : query_tokens.size(1), :]\n","\n","            else:\n","                query_outputs = self.qformer(\n","\n","                    query_embeds=query_tokens,\n","                    encoder_hidden_states=image_embeds,\n","                    encoder_attention_mask=image_attention_mask,\n","                    return_dict=True,\n","                )\n","                query_output = query_outputs.last_hidden_state            \n","            language_model_inputs = self.language_projection(query_output)\n","\n","            if attention_mask is None:\n","                attention_mask = torch.ones_like(input_ids)\n","                attention_mask = attention_mask.to(language_model_inputs.device)\n","\n","            inputs_embeds = self.get_input_embeddings()(input_ids)\n","            image_embeds_index = torch.where(input_ids == sp_token)\n","\n","            replace_length = inputs_embeds[image_embeds_index].shape[0]\n","        \n","            insert_embeds = language_model_inputs.reshape(-1,language_model_inputs.shape[-1])\n","\n","            insert_length = insert_embeds.shape[0]\n","        \n","            if insert_embeds.dtype != inputs_embeds.dtype:\n","                insert_embeds = insert_embeds.to(inputs_embeds.dtype)\n","\n","            if replace_length == insert_length:\n","                inputs_embeds[image_embeds_index] = insert_embeds\n","\n","            elif insert_length> replace_length:\n","                print(f\"shape mismatch leads to truncate. insert embedding tensor of shape {insert_embeds.shape} cannot be broadcast to replace placeholder of shape {inputs_embeds[image_embeds_index].shape}\")\n","                counts = Counter(image_embeds_index[0].cpu().detach().numpy())\n","                img_token_szie = 32\n","                # Sort the counts in ascending order of the numbers\n","                sorted_counts = sorted(counts.items())\n","\n","                # Extract the counts as a list\n","                result = [ (number,count) for number, count in sorted_counts]\n","                index = 0\n","                insert_embeds_list= []\n","\n","                img_idx = 0\n","                for num,each in result:\n","                    i_count = img_count[img_idx]\n","                    if img_idx != num: # skip sp_token\n","                        index+=i_count*img_token_szie\n","                        img_idx+=1\n","                        continue\n","                    if int(each / img_token_szie) == i_count:\n","                        insert_embeds_list.append(insert_embeds[index:index+each])\n","                        index+=each\n","                    else:\n","                        insert_embeds_list.append(insert_embeds[index:index+each])\n","                        index+= i_count*img_token_szie\n","                    img_idx +=1\n","                        \n","                insert_embeds = torch.concat(insert_embeds_list, dim=0)\n","                try:\n","                    inputs_embeds[image_embeds_index] = insert_embeds\n","                except:\n","                    print(f\"shape mismatch leads to HARD truncate. Require to fix the code! Insert embedding tensor of shape {insert_embeds.shape} cannot be broadcast to replace placeholder of shape {inputs_embeds[image_embeds_index].shape}\")\n","                    inputs_embeds[image_embeds_index] = insert_embeds[:replace_length]\n","\n","            # inputs_embeds[image_embeds_index] = language_model_inputs.reshape(-1,language_model_inputs.shape[-1])\n","\n","        min_padding_size = min(input_ids.shape[-1],self.max_sequence_length)\n","\n","        if not set_min_padding_size:\n","            min_padding_size = 10000 \n","            \n","        if self.config.use_decoder_only_language_model:\n","        # vicuna\n","            inputs_embeds = inputs_embeds[:,-min_padding_size:]\n","            attention_mask = attention_mask[:,-min_padding_size:]\n","        # t5\n","        else:\n","            inputs_embeds = inputs_embeds[:,:min_padding_size]\n","            attention_mask = attention_mask[:,:min_padding_size]\n","\n","\n","\n","        # concatenate query embeddings with prompt embeddings\n","\n","        outputs = self.language_model.generate(\n","            inputs_embeds=inputs_embeds,\n","            attention_mask=attention_mask,\n","            **generate_kwargs,\n","        )\n","\n","        return outputs"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4652364,"sourceId":7917611,"sourceType":"datasetVersion"},{"datasetId":4689975,"sourceId":7970660,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
