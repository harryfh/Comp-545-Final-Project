{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7917611,"sourceType":"datasetVersion","datasetId":4652364},{"sourceId":8197343,"sourceType":"datasetVersion","datasetId":4842738}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Modified from https://github.com/McGill-NLP/imagecode/blob/main/baselines/clip/evaluate_clip.py","metadata":{}},{"cell_type":"markdown","source":"## Evaluating Clip Non-Contrastive and Contrastive","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision \n!pip install git+https://github.com/openai/CLIP.git \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport json\nfrom collections import defaultdict\nfrom glob import glob\nimport os\nimport numpy as np\nimport clip\nimport torch\nfrom PIL import Image\nfrom pathlib import Path\nimport statistics\nimport argparse\n\ndef encode_images(photos_batch):\n    photos = [Image.open(photo_file) for photo_file in photos_batch]\n    photos_preprocessed = torch.stack([preprocess(photo) for photo in photos]).to(device)\n\n    with torch.no_grad():\n        photos_features = model.encode_image(photos_preprocessed)\n        photos_features /= photos_features.norm(dim=-1, keepdim=True)\n    return photos_features.cpu().numpy()\n\n\ndef encode_text(search_query):\n    with torch.no_grad():\n        text_encoded = model.encode_text(clip.tokenize(search_query, truncate=True).to(device))\n        text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n    return text_encoded.cpu().numpy()\n\n\ndef find_best_matches(text_features, photo_features):\n    similarities = (photo_features @ text_features.T).squeeze(1)\n    best_photo_idx = (-similarities).argsort()\n    similarities = -similarities\n    similarities.sort()\n    return best_photo_idx, similarities\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f'USING DEVICE: {device}')\nmodel, preprocess = clip.load('ViT-B/16', device=device, jit=False )  # Must set jit=False for training\n\ncheckpoint = torch.load('/kaggle/input/imagecode-checkpoints/NOCONTRA_clip_best__36_4e-6_ViT-B16_--job_id_1258699.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(checkpoint['epoch'])\nclip.model.convert_weights(model)  # Actually this line is unnecessary since clip by default already on float16\nmodel.eval()\n\ncorrect = 0\ntotal = 0\nvid_correct = 0\nvid_total = 0\nimg_correct= 0\nimg_total = 0\n\n\nimg_dirs = '/kaggle/input/imagecode-simple/image-sets/image-sets'\ndescriptions = json.load(open('/kaggle/input/imagecode-simple/valid_simple.json', 'r'))\n\nvalid = []\nfor data in descriptions:\n    valid.append((data['directory'], data['pos_idx'],data['neg_idx'], data['caption']))\n\nresults = defaultdict(dict)\nfor img_dir, pos_idx, neg_idx, text in tqdm(valid):\n    text = [text]\n    pos_idx = int(pos_idx)\n    img_files = list((Path(img_dirs) / img_dir).glob(\"*.jpg\"))\n    img_files = sorted(img_files, key=lambda x: int(str(x).split('/')[-1].split('.')[0][3:]))\n    img_files = [img_files[pos_idx], img_files[neg_idx]]\n    \n    images = [Image.open(photo_file) for photo_file in img_files]\n    images = torch.stack([preprocess(photo) for photo in images]).to(device)\n    text = clip.tokenize(text, truncate=True).to(device)\n    with torch.no_grad():\n        image_features = model.encode_image(images)\n        text_features = model.encode_text(text)\n        # normalized features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        logits = (image_features @ text_features.T).squeeze(1)\n    \n    pred = torch.argmax(logits).squeeze()\n\n\n    if pred.item() == 0:\n        correct += 1\n    if 'open-images' in img_dir:\n        img_total += 1\n        if pred.item() == 0:\n            img_correct += 1\n    else:\n        vid_total += 1\n        if pred.item() == 0:\n            vid_correct += 1     \n    results[img_dir].update({f'raw_preds_{pos_idx}': logits.squeeze().tolist(), f'clip_pred_{pos_idx}': int(pred.item()) ,f'correct_{pos_idx}': 1 if pos_idx == pred else 0})\n\n\nprint('OVERALL ACC: ' + str(round(correct/len(valid),4)))\nprint('VIDEO ACC: ' + str(round(vid_correct/vid_total,4)))\nprint('IMG ACC: ' + str(round(img_correct/img_total,4)))\njson.dump(results, open(f'nocontra-clip-valid-simple-data.json', 'w'), indent=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T18:16:40.967862Z","iopub.execute_input":"2024-04-22T18:16:40.968835Z","iopub.status.idle":"2024-04-22T18:18:51.423888Z","shell.execute_reply.started":"2024-04-22T18:16:40.968796Z","shell.execute_reply":"2024-04-22T18:18:51.422915Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"USING DEVICE: cuda\n2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2302/2302 [01:49<00:00, 20.99it/s]","output_type":"stream"},{"name":"stdout","text":"OVERALL ACC: 0.6338\nVIDEO ACC: 0.5908\nIMG ACC: 0.8209\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluating Contextual and Temporal Contextual\n\nHere we augment the evaluation_contextual.py in a similiar way that we did above. Instead of a classification task with 10 images, the problem is reduced to a binary classification.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/McGill-NLP/imagecode.git","metadata":{"execution":{"iopub.status.busy":"2024-04-22T17:18:09.135116Z","iopub.execute_input":"2024-04-22T17:18:09.135524Z","iopub.status.idle":"2024-04-22T17:18:12.025955Z","shell.execute_reply.started":"2024-04-22T17:18:09.135490Z","shell.execute_reply":"2024-04-22T17:18:12.024625Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cloning into 'imagecode'...\nremote: Enumerating objects: 589, done.\u001b[K\nremote: Counting objects: 100% (55/55), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 589 (delta 45), reused 37 (delta 37), pack-reused 534\u001b[K\nReceiving objects: 100% (589/589), 18.64 MiB | 18.63 MiB/s, done.\nResolving deltas: 100% (290/290), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# inspired from: https://github.com/openai/CLIP/issues/83\n# https://github.com/openai/CLIP/issues/83\nimport json\nimport os\nimport random\nimport wandb\nimport clip\nfrom clip import model\nimport torch\nfrom torch import autograd\nimport tqdm\nfrom torch import nn, optim\nfrom PIL import Image\nfrom pathlib import Path\nfrom collections import defaultdict\nimport sys\nimport argparse\nprint(os.getcwd())\nfrom volta_src.config import BertConfig\nfrom volta_src.embeddings import BertLayerNorm\nfrom volta_src.encoders import GeLU\nfrom extras import convert_sents_to_features, BertLayer\n\n\nrandom.seed(10)\ntorch.manual_seed(10)\n\ndef find_best_matches(text_features, photo_features):\n    similarities = (photo_features @ text_features.T).squeeze(1)\n    best_photo_idx = (-similarities).argsort()\n    similarities = -similarities\n    similarities.sort()\n    return best_photo_idx, similarities\n\n\ndef convert_models_to_fp32(model):\n    for p in model.parameters():\n        if p.grad is not None:\n            p.data = p.data.float()\n            p.grad.data = p.grad.data.float()\n\nclass ContextualCLIP(torch.nn.Module):\n    def __init__(self, bert_config, args):\n        super(ContextualCLIP, self).__init__()\n        self.clip, self.preprocess = clip.load('ViT-B/16', device=device, jit=False)\n        config = BertConfig.from_dict(bert_config)\n        self.fusion = args.fusion\n        if self.fusion == 'concat':\n            hidden_size = 1024\n        else:\n            hidden_size = 512\n\n        config.hidden_size =  hidden_size\n        config.num_attention_heads = 8\n        self.transformer = nn.ModuleList([BertLayer(config) for _ in range(args.transformer_layers)])\n        self.transformer.cuda()\n        self.prediction_layer = nn.Linear(config.hidden_size, 1).cuda()\n        self.batch_size = 1\n        self.logit_scale = float(args.logit_scale)\n        self.frozen_clip = args.frozen_clip\n        self.add_input = args.add_input\n        self.positional = args.positional\n        if args.positional:\n            self.positional_emb = torch.nn.Embedding(10,hidden_size).cuda()\n\n    def forward(self, images, text, pos_mask):\n        if self.frozen_clip:\n            with torch.no_grad():\n                image_features = self.clip.encode_image(images)\n                text_features = self.clip.encode_text(text)\n        else:\n            image_features = self.clip.encode_image(images)\n            text_features = self.clip.encode_text(text)\n        # normalized features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        text_features = torch.cat(10 * [text_features])\n        if self.fusion == 'concat':\n            x = torch.cat((image_features, text_features), dim=1)\n        else:\n            x = (self.logit_scale * image_features) * text_features\n        x_ = torch.unsqueeze(x,dim=0)\n        if self.positional:\n            embs = self.positional_emb(torch.arange(10).cuda())\n            embs = embs * pos_mask\n            x_pos = x_ + embs\n        else:\n            x_pos = x_\n        attention_mask = torch.ones((self.batch_size,1,1,10)).cuda()\n        x = self.transformer[0](x_pos, attention_mask)\n        for layer_module in self.transformer[1:]:\n            x = layer_module(x, attention_mask) #TODO: remove hard-coding of 10\n        if self.add_input:\n            x = x + x_\n        preds = self.prediction_layer(x.half())\n        return preds\n\n    def encode_images(self, photos_batch):\n        photos = [Image.open(photo_file) for photo_file in photos_batch]\n        photos_preprocessed = torch.stack([self.preprocess(photo) for photo in photos]).to(device)\n\n        with torch.no_grad():\n            photos_features = self.clip.encode_image(photos_preprocessed)\n            photos_features /= photos_features.norm(dim=-1, keepdim=True)\n        return photos_features.cpu().numpy()\n\n    def encode_text(self, search_query):\n        with torch.no_grad():\n            text_encoded = self.clip.encode_text(clip.tokenize(search_query, truncate=True).to(device))\n            text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n        return text_encoded.cpu().numpy()\n\n\n\nARGS = {\n    'checkpoint': '/kaggle/input/imagecode-checkpoints/TEMP-CONTEXTUAL_clip_best__36_0.0001_2e-06_ViT-B16_mult_gelu_1000_True_True_True_True_1.0_1.0_2_1252131.pt',\n    'test_descr_path': '/kaggle/input/imagecode-simple/valid_simple.json',\n    'imgs_path': '/kaggle/input/imagecode-simple/image-sets/image-sets',\n    'batchsize': 36,\n    'fusion': 'mult',\n    'activation': 'gelu',\n    'logit_scale': 1000,\n    'frozen_clip': True,\n    'add_input': True,\n    'positional': True,\n    'head_scheduler': 1.0,\n    'base_scheduler': 1.0,\n    'transformer_layers': 2\n}\nargs = argparse.Namespace(**ARGS)\nassert args.fusion in ['concat', 'mult']\nassert args.activation in ['leaky-relu', 'relu', 'gelu']\n\n\nimg_dirs = args.imgs_path\nvalid_data = json.load(open(args.test_descr_path, 'r'))\nvalid = []\nfor data in valid_data:\n    valid.append((data['directory'], data['pos_idx'],data['neg_idx'], data['caption']))\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f'DEVICE USED: {device}')\n\nbert_config = json.load(open('vilbert-and-bert-config.json', 'r'))\ncontextual_clip = ContextualCLIP(bert_config, args)\ncheckpoint = torch.load(args.checkpoint)\ncontextual_clip.load_state_dict(checkpoint['model_state_dict'])\n\n\nif device == \"cpu\":\n    contextual_clip.float()\nelse:\n    clip.model.convert_weights(\n        contextual_clip)  # Actually this line is unnecessary since clip by default already on float16\n\n\ncorrect = 0\ntotal = 0\nvid_correct = 0\nvid_total = 0\nimg_correct= 0\nimg_total = 0\n\nresults = defaultdict(dict)\nfor img_dir, pos_idx, neg_idx, text in tqdm.tqdm(valid):\n    text = [text]\n    img_idx = int(pos_idx)\n    img_files = list((Path(img_dirs) / img_dir).glob(\"*.jpg\"))\n    img_files = sorted(img_files, key=lambda x: int(str(x).split('/')[-1].split('.')[0][3:]))\n    # Only a binary case for simple, instead of changing anything, just repeat the same image 5 times to\n    # maintain the original positional embeddings from the checkpoints provided\n    img_files = [img_files[pos_idx]] * 5 + [img_files[neg_idx]] * 5 \n    images = [Image.open(photo_file) for photo_file in img_files]\n    images = torch.stack([contextual_clip.preprocess(photo) for photo in images]).to(device)\n    text = clip.tokenize(text, truncate=True).to(device)\n    if \"open-images\" in str(img_dir):\n        pos_mask = torch.zeros((10,1)).cuda() # Change 10 to 1\n    else:\n        pos_mask = torch.ones((10,1)).cuda() # Change 10 to 1\n    with torch.no_grad():\n        logits = contextual_clip(images, text, pos_mask).squeeze()\n    pred = torch.argmax(logits).squeeze()\n    # Correct is always in the first half now\n    if pred.item() < 5:\n        correct += 1\n    if 'open-images' in img_dir:\n        img_total += 1\n        if pred.item() < 5:\n            img_correct += 1\n    else:\n        vid_total += 1\n        if pred.item() < 5:\n            vid_correct += 1     \n    total += 1\n    results[img_dir].update({f'raw_preds_{img_idx}': logits.squeeze().tolist(), f'clip_pred_{img_idx}': int(pred.item()) ,f'correct_{img_idx}': 1 if img_idx == pred else 0})\n\nprint('OVERALL ACC: ' + str(round(correct/len(valid),4)))\nprint('VIDEO ACC: ' + str(round(vid_correct/vid_total,4)))\nprint('IMG ACC: ' + str(round(img_correct/img_total,4)))\n\njson.dump(results, open(f'CONTEXTUAL_valid_simple.json', 'w'), indent=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:26:36.879375Z","iopub.execute_input":"2024-04-22T18:26:36.879794Z","iopub.status.idle":"2024-04-22T18:33:04.012459Z","shell.execute_reply.started":"2024-04-22T18:26:36.879762Z","shell.execute_reply":"2024-04-22T18:33:04.011506Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"/kaggle/working/imagecode/baselines/clip\nDEVICE USED: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2302/2302 [06:22<00:00,  6.02it/s]","output_type":"stream"},{"name":"stdout","text":"OVERALL ACC: 0.7033\nVIDEO ACC: 0.6587\nIMG ACC: 0.8977\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}